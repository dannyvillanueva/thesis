@book{Shai2014,
    title={Understanding Machine Learning: From Theory to Algorithms},
    author={Shai Shalev-Shwartz and Shai Ben-David},
    publisher={Cambridge University Press},
    year={2014}
}

@book{Nilsson1998,
    title={Introduction to Machine Learning},
    author={Nils J. Nilson},
    publisher={Stanford University},
    url = {http://ai.stanford.edu/~nilsson/MLBOOK.pdf},
    year={1998}
}

@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    url = {http://www.deeplearningbook.org},
    year={2016}
}

@book{Russell2010, 
place={Upper Saddle River}, 
title={Artificial intelligence: a modern approach}, 
publisher={Pearson}, 
author={Russell, Stuart J. and Norvig, Peter}, 
year={2010}
}

@book{Kelleher2015, 
title={Fundamentals of Machine Learning for Predictive Data Analytics}, 
publisher={The MIT Press}, 
author={John D. Kelleher, Brian Mac Namee, Aoife D’Arcy}, 
year={2015}
}

@book{Nevala2017,
    title={The Machine Learning Primer},
    author={ Kimberly Nevala},
    publisher={SAS Institute Inc.},
    year={2016}
}

@book{Gurney2004,
    title={An Introduction to Neural Networks},
    author={Kevin Gurney},
    publisher={Taylor and Francis e-Library},
    year={2004}
}

@book{Kriesel2005,
    title = {A Brief Introduction to  Neural Networks},
    author={David Kriesel},
    publisher={dkriesel},
    year={2005}
}

@book{Indurkhya2010,
    title = {Handbook of Natural Language Processing},
    author={Nitin Iindukhya Fred J. Damerau},
    publisher={CRC Press},
    year={2010}
}

@INPROCEEDINGS{Halibas2018, 
author={A. S. Halibas and A. S. Shaffi and M. A. K. V. Mohamed}, 
booktitle={2018 Majan International Conference (MIC)}, 
title={Application of text classification and clustering of Twitter data for business analytics}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={In the recent years, social networks in business are gaining unprecedented popularity because of their potential for business growth. Companies can know more about consumers' sentiments towards their products and services, and use it to better understand the market and improve their brand. Thus, companies regularly reinvent their marketing strategies and campaigns to fit consumers' preferences. Social analysis harnesses and utilizes the vast volume of data in social networks to mine critical data for strategic decision making. It uses machine learning techniques and tools in determining patterns and trends to gain actionable insights. This paper selected a popular food brand to evaluate a given stream of customer comments on Twitter. Several metrics in classification and clustering of data were used for analysis. A Twitter API is used to collect twitter corpus and feed it to a Binary Tree classifier that will discover the polarity lexicon of English tweets, whether positive or negative. A k-means clustering technique is used to group together similar words in tweets in order to discover certain business value. This paper attempts to discuss the technical and business perspectives of text mining analysis of Twitter data and recommends appropriate future opportunities in developing this emerging field.}, 
keywords={application program interfaces;business data processing;data mining;decision making;learning (artificial intelligence);pattern clustering;social networking (online);text analysis;text classification;business analytics;social networks;business growth;marketing strategies;social analysis harnesses;strategic decision making;machine learning techniques;Twitter API;twitter corpus;clustering technique;business value;technical business;text mining analysis;Twitter data clustering;critical data mining;food brand;Twitter;Sentiment analysis;Decision trees;Companies;Text categorization;Twitter;Sentiment Analysis;Decision Tree;k-means;Social Media}, 
doi={10.1109/MINTC.2018.8363162}, 
ISSN={}, 
month={March},}

@INPROCEEDINGS{Long2015, 
author={J. Long and E. Shelhamer and T. Darrell}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Fully convolutional networks for semantic segmentation}, 
year={2015}, 
volume={}, 
number={}, 
pages={3431-3440}, 
abstract={Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.}, 
keywords={image classification;image segmentation;inference mechanisms;learning (artificial intelligence);fully convolutional networks;semantic segmentation;visual models;pixels-to-pixels;inference;learning;contemporary classification networks;PASCAL VOC;NYUDv2;SIFT flow;Semantics;Training;Convolution;Image segmentation;Computer architecture;Deconvolution;Adaptation models}, 
doi={10.1109/CVPR.2015.7298965}, 
ISSN={1063-6919}, 
month={June},}

@misc{Graves2017, 
title={Supervised Sequence Labelling with Recurrent Neural Networks}, url={https://www.cs.toronto.edu/~graves/preprint.pdf}, 
publisher={University of Toronto Press}, 
author={Alex Graves},
year={2010},
}

@INPROCEEDINGS{Xavier2018, 
author={G. M. Xavier and J. M. de Seixas}, 
booktitle={2018 International Joint Conference on Neural Networks (IJCNN)}, 
title={Fault Detection and Diagnosis in a Chemical Process using Long Short-Term Memory Recurrent Neural Network}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-8}, 
abstract={Long short-term memory recurrent neural networks have been proved to be especially useful for learning sequences comprising longer-term patterns of unknown length as they are able to preserve long-term memory. The learning of higher level temporal features could be achieved by stacking recurrent hidden layers for faster learning with sparser representations. In this work, we propose a novel approach to data driven fault detection and diagnosis of a chemical process. The method employs a state-of-the-art deep-learning technique, viz. the long short-term memory recurrent neural network. An application of the proposed approach is performed with realistic simulated data from a chemical process literature benchmark. Results point out an excellent performance when compared to already published linear and nonlinear fault detection and diagnosis methods.}, 
keywords={chemical engineering computing;fault diagnosis;learning (artificial intelligence);recurrent neural nets;chemical process;long short-term memory recurrent neural network;longer-term patterns;recurrent hidden layers;data driven fault detection;deep-learning technique;fault diagnosis;higher level temporal features;Recurrent neural networks;Feeds;Process control;Chemical processes;Inductors;Cooling;deep networks;recurrent neural network;long short-term memory;fault detection and diagnosis;Tennessee Eastman chemical process}, 
doi={10.1109/IJCNN.2018.8489385}, 
ISSN={2161-4407}, 
month={July},}

@INPROCEEDINGS{Chandra2017, 
author={B. Chandra and R. K. Sharma}, 
booktitle={2017 International Joint Conference on Neural Networks (IJCNN)}, 
title={On improving recurrent neural network for image classification}, 
year={2017}, 
volume={}, 
number={}, 
pages={1904-1907}, 
abstract={The paper proposes a new method for improving the performance of Recurrent Neural Networks. The proposed method uses two parallel recurrent layers which execute independent of each other. The final output of recurrent layer at any time step is computed as the mean of the modulus of the output of these two layers. The proposed method attempts to overcome the limitations of the existing Recurrent Neural Networks with regard to the flow of gradient. Comparative performance of the proposed method has been carried out with Long Short Term Memory (LSTM) and Identity initialized RNN (IRNN), the latest improved version of RNN for classification of images. On benchmark image datasets, it has been shown that the proposed method outperforms both IRNN and LSTM.}, 
keywords={image classification;recurrent neural nets;recurrent neural network;image classification;parallel recurrent layers;long short term memory;LSTM;identity initialized RNN;IRNN;benchmark image datasets;Recurrent neural networks;Neurons;Logic gates;Training;Standards;Backpropagation;Benchmark testing;Recurrent Neural Network;Deep Learning;Activation Function}, 
doi={10.1109/IJCNN.2017.7966083}, 
ISSN={2161-4407}, 
month={May},}

@INPROCEEDINGS{Dai2017, 
author={X. Dai and M. Bikdash and B. Meyer}, 
booktitle={SoutheastCon 2017}, 
title={From social media to public health surveillance: Word embedding based clustering method for twitter classification}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-7}, 
abstract={Social media provide a low-cost alternative source for public health surveillance and health-related classification plays an important role to identify useful information. In this paper, we summarized the recent classification methods using social media in public health. These methods rely on bag-of-words (BOW) model and have difficulty grasping the semantic meaning of texts. Unlike these methods, we present a word embedding based clustering method. Word embedding is one of the strongest trends in Natural Language Processing (NLP) at this moment. It learns the optimal vectors from surrounding words and the vectors can represent the semantic information of words. A tweet can be represented as a few vectors and divided into clusters of similar words. According to similarity measures of all the clusters, the tweet can then be classified as related or unrelated to a topic (e.g., influenza). Our simulations show a good performance and the best accuracy achieved was 87.1%. Moreover, the proposed method is unsupervised. It does not require labor to label training data and can be readily extended to other classification problems or other diseases.}, 
keywords={diseases;health care;learning (artificial intelligence);medical computing;natural language processing;pattern classification;pattern clustering;social networking (online);vectors;word processing;social media;public health surveillance;word embedding based clustering method;Twitter classification;health-related classification;bag-of-words;BOW model;text semantic meaning;natural language processing;NLP;optimal vector learning;word clusters;diseases;Public healthcare;Natural language processing;Diseases;Support vector machines;Surveillance;Twitter;Unsupervised Classification;Public Health;Twitter;Social Network;Big data;Surveillance;Word Embeddings;Word2Vec;Machine learning;Natural Language Processing;Clustering Process;Similarity Measure}, 
doi={10.1109/SECON.2017.7925400}, 
ISSN={1558-058X}, 
month={March},}

@INPROCEEDINGS{Ahuja2017, 
author={S. Ahuja and G. Dubey}, 
booktitle={2017 2nd International Conference on Telecommunication and Networks (TEL-NET)}, 
title={Clustering and sentiment analysis on Twitter data}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={Twitter is a social media platform is a great place where people from all parts of the world can make their opinions heard. Twitter produces around 500 million of tweets daily which amounts to about 8TB of data. The data generated in twitter can be very useful if analyzed as we can extract important information via opinion mining. Opinions about any news or launch of a product or a certain kind of trend can be observed well in twitter data. The main aim of sentiment analysis (or opinion mining) is to discover emotion, opinion, subjectivity and attitude from a natural text. In twitter sentiment analysis, we categorize tweets into positive and negative sentiment. Clustering is a protean procedure in which identically resembled objects are grouped together and form a pack or cluster. We conducted a study and found out that the use of clustering can quickly and efficiently distinguish tweets on the basis of their sentiment scores and can find weekly and strongly positive or negative tweets when clustered with results of different dictionaries. This paper surveys different approaches of clustering with respect to sentiment analysis and presents a way to find relationships between the tweets on the basis of polarity and subjectivity.}, 
keywords={data mining;pattern clustering;sentiment analysis;social networking (online);social media platform;positive sentiment;negative sentiment;Twitter data;Twitter sentiment analysis;opinion mining;clustering;Clustering algorithms;Sentiment analysis;Twitter;Data mining;Dictionaries;Classification algorithms;Elbow;Cluster;Opinions;Sentiments;Twitter}, 
doi={10.1109/TEL-NET.2017.8343568}, 
ISSN={}, 
month={Aug},}

@INPROCEEDINGS{Wang2008, 
author={Haixun Wang and Jian Pei}, 
booktitle={JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY}, 
title={Clustering by Pattern Similarity}, 
year={2008}, 
pages={481-496}, 
abstract={The task of clustering is to identify classes of similar objects among a set of objects. The deﬁnition of similarity varies from one clustering model to another. However, in most of these models the concept of similarity is often based on such metrics as Manhattan distance, Euclidean distance or other Lp distances. In other words, similar objects must have close values in at least a set of dimensions. In this paper, we explore a more general type of similarity. Under the pCluster model we proposed, two objects are similar if they exhibit a coherent pattern on a subset of dimensions. The new similarity concept models a wide range of applications. For instance, in DNA microarray analysis, the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very much alike. Discovery of such clusters of genes is essential in revealing signiﬁcant connections in gene regulatory networks. E-commerce applications, such as collaborative ﬁltering, can also beneﬁt from the new model, because it is able to capture not only the closeness of values of certain leading indicators but also the closeness of (purchasing, browsing, etc.) patterns exhibited by the customers. In addition to the novel similarity model, this paper also introduces an eﬀective and eﬃcient algorithm to detect such clusters, and we perform tests on several real and synthetic data sets to show its performance.}, 
keywords={data mining, clustering, pattern similarity}, 
ISSN={}, 
month={May},}

@INPROCEEDINGS{Shalini2018, 
author={Shalini L and Gopali Naga Sravya}, 
booktitle={International Research Journal of Engineering and Technology (IRJET) }, 
title={Analysis of Health-Tweets using K-means clustering}, 
year={2018},
volume={05}, 
number={03}, 
pages={2074-2077}, 
month={Mar},}

@misc{Brownlee2017, 
title={A Gentle Introduction to the Bag-of-Words Model}, 
url={https://machinelearningmastery.com/gentle-introduction-bag-words-model/}, 
journal={Deep Learning for Natural Language Processing}, 
publisher={Machine Learning Mastery}, 
author={Jason Brownlee},
year={2017},
month={10}
}

@inproceedings{Tripathy2014,
 author = {Tripathy, Rudra M. and Sharma, Shashank and Joshi, Sachindra and Mehta, Sameep and Bagchi, Amitabha},
 title = {Theme Based Clustering of Tweets},
 booktitle = {Proceedings of the 1st IKDD Conference on Data Sciences},
 series = {CoDS '14},
 year = {2014},
 isbn = {978-1-4503-2475-5},
 location = {Delhi, India},
 pages = {7:1--7:5},
 articleno = {7},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/2567688.2567694},
 doi = {10.1145/2567688.2567694},
 acmid = {2567694},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Clustering, Social Networks, Twitter, Wikipedia},
} 

@INPROCEEDINGS{Lin2013, 
author={E. Lin and S. Fang and J. Wang}, 
booktitle={2013 27th International Conference on Advanced Information Networking and Applications Workshops}, 
title={Mining Online Book Reviews for Sentimental Clustering}, 
year={2013}, 
volume={}, 
number={}, 
pages={179-184}, 
abstract={The classification of consumable media by mining relevant text for their identifying features is a subjective process. Previous attempts to perform this type of feature mining have generally been limited in scope due to having limited access to user data. Many of these studies used human domain knowledge to evaluate the accuracy of features extracted using these methods. In this paper, we mine book review text to identify nontrivial features of a set of similar books. We make comparisons between books by looking for books that share characteristics, ultimately performing clustering on the books in our data set. We use the same mining process to identify a corresponding set of characteristics in users. Finally, we evaluate the quality of our methods by examining the correlation between our similarity metric, and user ratings.}, 
keywords={data mining;pattern classification;pattern clustering;publishing;text analysis;online book review mining;sentimental clustering;consumable media classification;text mining;feature mining;human domain knowledge;similarity metric;user rating;Book reviews;Correlation;Text mining;Databases;Computers;Knowledge discovery;clustering;online reviews;text mining;sentiment analysis;clustering}, 
doi={10.1109/WAINA.2013.172}, 
ISSN={}, 
month={March},}

@INPROCEEDINGS{Cheong2010, 
author={M. Cheong and V. Lee}, 
booktitle={2010 20th International Conference on Pattern Recognition}, 
title={A Study on Detecting Patterns in Twitter Intra-topic User and Message Clustering}, 
year={2010}, 
volume={}, 
number={}, 
pages={3125-3128}, 
abstract={Timely detection of hidden patterns is the key for the analysis and estimating of driving determinants for mission critical decision making. This study applies Cheong and Lee's “context-aware” content analysis framework to extract latent properties from Twitter messages (tweets). In addition, we incorporate an unsupervised Self-organizing Feature Map (SOM) as a machine learning-based clustering tool that has not been investigated in the context of opinion mining and sentimental analysis using microblogging. Our experimental results reveal the detection of interesting patterns for topics of interest which are latent and cannot be easily detected from the observed tweets without the aid of machine learning tools.}, 
keywords={decision making;pattern clustering;social networking (online);pattern detection;twitter intratopic user;message clustering;decision making;context-aware content analysis framework;Twitter messages;self-organizing feature map;machine learning-based clustering tool;opinion mining;sentimental analysis;microblogging;Twitter;Visualization;Media;Nominations and elections;Communities;Clustering algorithms;Online documents;Group interaction: analysis of verbal and non-verbal communication;Pattern recognition systems and applications}, 
doi={10.1109/ICPR.2010.765}, 
ISSN={1051-4651}, 
month={Aug},}

@misc{Tensor2019, 
title={Get Started with TensorFlow}, 
url={https://www.tensorflow.org/tutorials/}, 
journal={TensorFlow}, 
author={Google},
year={2019}
}

@misc{Keras2019, 
title={Keras: The Python Deep Learning library}, 
url={https://keras.io/}, 
journal={Keras Documentation}, 
author={Keras},
year={2019}
}

@misc{Cuda2019,
title={NVIDIA Accelerated Computing: CUDA Zone}, 
url={https://developer.nvidia.com/cuda-zone/}, 
journal={CUDA Documentation}, 
author={NVIDIA},
year={2019}
}

@misc{Hadoop2019,
title={Apache Hadoop}, 
url={https://hadoop.apache.org/}, 
journal={Apache hadoop Documentation}, 
author={Apache},
year={2019}
}

@misc{Hive2019,
title={Getting Started With Apache Hive Software}, 
url={https://hive.apache.org/}, 
journal={Apache Hive TM}, 
author={Apache},
year={2019}
}

@misc{Kafka2019,
title={Apache Kafka® is a distributed streaming platform. What exactly does that mean?}, 
url={https://kafka.apache.org/intro}, 
journal={Apache Kafka Introduction}, 
author={Apache},
year={2019}
}

@misc{Spark2019,
title={Spark Overview}, 
url={https://spark.apache.org/docs/latest/}, 
journal={Apache Spark Overview}, 
author={Apache},
year={2019}
}

@misc{Torch2019,
title={A scientific computing framework for LuaJIT}
url = {http://torch.ch/}
journal={Getting started with Torch}, 
author={Ronan, Clément, Koray and Soumith},
year={2019}
}

@article{Mikolov2013,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, G.s and Dean, Jeffrey},
year = {2013},
month = {10},
pages = {},
title = {Distributed Representations of Words and Phrases and their Compositionality},
volume = {26},
journal = {Advances in Neural Information Processing Systems}
}

@inproceedings{Pennington2014,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@article{Kiros2015,
  author    = {Ryan Kiros and
               Yukun Zhu and
               Ruslan Salakhutdinov and
               Richard S. Zemel and
               Antonio Torralba and
               Raquel Urtasun and
               Sanja Fidler},
  title     = {Skip-Thought Vectors},
  journal   = {CoRR},
  volume    = {abs/1506.06726},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.06726},
  archivePrefix = {arXiv},
  eprint    = {1506.06726},
  timestamp = {Mon, 13 Aug 2018 16:48:27 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KirosZSZTUF15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Conneau2017,
  author    = {Alexis Conneau and
               Douwe Kiela and
               Holger Schwenk and
               Lo{\"{\i}}c Barrault and
               Antoine Bordes},
  title     = {Supervised Learning of Universal Sentence Representations from Natural
               Language Inference Data},
  journal   = {CoRR},
  volume    = {abs/1705.02364},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.02364},
  archivePrefix = {arXiv},
  eprint    = {1705.02364},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ConneauKSBB17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Cer2018,
  author    = {Daniel Cer and
               Yinfei Yang and
               Sheng{-}yi Kong and
               Nan Hua and
               Nicole Limtiaco and
               Rhomni St. John and
               Noah Constant and
               Mario Guajardo{-}Cespedes and
               Steve Yuan and
               Chris Tar and
               Yun{-}Hsuan Sung and
               Brian Strope and
               Ray Kurzweil},
  title     = {Universal Sentence Encoder},
  journal   = {CoRR},
  volume    = {abs/1803.11175},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.11175},
  archivePrefix = {arXiv},
  eprint    = {1803.11175},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-11175},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Ganesan2015, 
title = {What is text similarity?}, 
url = {kavita-ganesan.com/what-is-text-similarity}, 
journal = {Machine Learning, NLP and Text Mining Expert}, 
author = {Kavita Ganesan},
year = {2015},
month = {11}
}

@article{Gomaa2013,
author = {Gomaa, Wael and Fahmy, Aly},
year = {2013},
month = {04},
pages = {},
title = {A Survey of Text Similarity Approaches},
volume = {68},
journal = {international journal of Computer Applications},
doi = {10.5120/11638-7118}
}

@article{Pradhan2015,
author = {Pradhan, Nitesh and Gyanchandani, Manasi and Wadhvani, Rajesh},
year = {2015},
month = {06},
pages = {29-34},
title = {A Review on Text Similarity Technique used in IR and its Application},
volume = {120},
journal = {International Journal of Computer Applications},
doi = {10.5120/21257-4109}
}

@article{Majumder2016,
author = {Majumder, Goutam and Pakray, Dr. Partha and Gelbukh, Alexander and Pinto, David},
year = {2016},
month = {12},
pages = {647-665},
title = {Semantic Textual Similarity Methods, Tools, and Applications: A Survey},
volume = {20},
journal = {Computacion y Sistemas},
doi = {10.13053/CyS-20-4-2506}
}

@INPROCEEDINGS{Zhang2015, 
author={S. {Zhang} and X. {Zheng} and C. {Hu}}, 
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
title={A survey of semantic similarity and its application to social network analysis}, 
year={2015}, 
volume={}, 
number={}, 
pages={2362-2367}, 
keywords={social networking (online);text analysis;semantic textual similarity;background information resource;text related research;semantic similarity measure;online social network analysis;similarity computation methods;Semantics;Social network services;Knowledge based systems;Encyclopedias;Electronic publishing;Internet;semantic similarity;semantic textual similarity;social network analysis}, 
doi={10.1109/BigData.2015.7364028}, 
ISSN={}, 
month={Oct},}

@MISC{Zhu2014,
    author = {Tian Tian Zhu and Man Lan},
    title = {ECNU: Leveraging on Ensemble of Heterogeneous Features and Information Enrichment for Cross Level Semantic Similarity Estimation},
    year = {2014}
}

@MISC{Jurafsky2018,
    author = {Dan Jurafsky and James H. Martin},
    title = {Speech and Language Processing},
    year = {2018}
}