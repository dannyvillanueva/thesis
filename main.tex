\documentclass[12pt]{report}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{float}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{textcomp}
\usepackage{lipsum}
\usepackage{color}
\usepackage{sectsty}
\usepackage[nottoc]{tocbibind}
\usepackage{grffile}
\usepackage{fancyhdr}
\usepackage{acro}
\usepackage[a4paper,includeheadfoot,margin=2.5cm]{geometry}
\pagestyle{fancy}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{dirtree}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{pgfplots}
\graphicspath{{images/}}
\renewcommand{\headrulewidth}{0pt}
\usepackage{listings}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{tabu}
\usepackage[scientific-notation=true]{siunitx}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{tikzsymbols}
\usepackage{makecell}
\usetikzlibrary{matrix,chains,positioning,decorations,arrows}
\emergencystretch=1em
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\newlist{steps}{enumerate}{1}
\setlist[steps, 1]{label = Step \arabic*:}
\newcommand\myicon[1]{{\color{#1}\rule{2ex}{2ex}}}
\newcommand{\myfolder}[2]{\myicon{#1}\ {#2}}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codeyellow}{rgb}{0.85, 0.65, 0.13}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour}, commentstyle=\color{codegreen},
	keywordstyle=\color{blue},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codeyellow},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

\usepackage[toc,page]{appendix}
%\usepackage[sorting=none]{biblatex}
%\addbibresource{ref.bib}

\DeclareAcronym{ML}{
  short = ML,
  long  = Machine Learning,
  class = abbrev
}
\DeclareAcronym{SL}{
  short = SL,
  long  = Supervised Learning,
  class = abbrev
}
\DeclareAcronym{UL}{
  short = UL,
  long  =Unsupervised Learning,
  class = abbrev
}
\DeclareAcronym{DL}{
  short = DL,
  long  = Deep Learning,
  class = abbrev
}
\DeclareAcronym{CNN}{
  short = CNN,
  long  = Convolutional Neural Network,
  class = abbrev
}
\DeclareAcronym{RNN}{
  short = RNN,
  long  = Recurrent Neural Network,
  class = abbrev
}
\DeclareAcronym{THS}{
  short = THS,
  long  = Twitter Health Surveillance,
  class = abbrev
}
\DeclareAcronym{HDFS}{
  short = HDFS,
  long  = Hadoop Distributed File System,
  class = abbrev
}
\DeclareAcronym{YARN}{
	short = YARN,
	long  = Yet Another Resource Negotiator,
	class = abbrev
}
\DeclareAcronym{NIH}{
  short = NIH,
  long  = National Institutes of Health,
  class = abbrev
}
\DeclareAcronym{AI}{
  short = AI,
  long  = Artificial Intelligence,
  class = abbrev
}
\DeclareAcronym{US}{
  short = US,
  long  = United States,
  class = abbrev
}
\DeclareAcronym{NSF}{
	short = NSF,
	long  = National Science Foundation,
	class = abbrev
}
\DeclareAcronym{CSV}{
  short = CSV,
  long  = Comma Separated Values,
  class = abbrev
}
\DeclareAcronym{API}{
  short = API,
  long  = Application Program Interface,
  class = abbrev
}
\DeclareAcronym{GPU}{
  short = GPU,
  long  = Graphic Processing Unit,
  class = abbrev
}
\DeclareAcronym{NLP}{
  short = NLP,
  long  = Natural Language Processing,
  class = abbrev
}
\DeclareAcronym{MLP}{
	short = MLP,
	long = Multilayer Perceptron,
	class = abbrev
}
\DeclareAcronym{UPRM}{
	short = UPRM,
	long  = University of Puerto Rico Mayag\"uez Campus,
	class = abbrev
}
\DeclareAcronym{LSTM}{
	short = LSTM,
	long  = Long Short-Term Memory,
	class = abbrev
}
\DeclareAcronym{GRU}{
	short = GRU,
	long  = Gated Recurrent Unit,
	class = abbrev
}
\DeclareAcronym{NLTK}{
	short = NLTK,
	long  = Natural Language Toolkit,
	class = abbrev
}
\DeclareAcronym{CUDA}{
	short = CUDA,
	long  = Compute Unified Device Architecture,
	class = abbrev
}
\DeclareAcronym{SQL}{
	short = SQL,
	long  = Structured Query Language,
	class = abbrev
}
\DeclareAcronym{CPU}{
	short = CPU,
	long  = Central Processing Unit,
	class = abbrev
}
\DeclareAcronym{OS}{
	short = OS,
	long  = Operating System,
	class = abbrev
}
\DeclareAcronym{JSON}{
	short = JSON,
	long  = JavaScript Object Notation,
	class = abbrev
}
\DeclareAcronym{ReLU}{
	short = ReLU,
	long = Rectified Linear Unit,
	class = abbrev
}

\pgfplotsset{compat=1.14}
\setlength{\headheight}{15pt}
\setcounter{secnumdepth}{3}
\begin{document}
\newtheorem{definition}{Example}

\newtheorem{property}{Property}

\pagenumbering{roman}%

\begin{titlepage}
    \centering 
    {\LARGE \textbf {Find Similar Tweets Within Health Related Topics}\par}
    \vspace{1cm}
    {By\par}
    {Danny Gilberto Villanueva Vega\par}    
    {A thesis submitted in partial fulfillment of the requirements for the degree \par}
    \vspace{.1cm}
    {of\par}
    \vspace{.1cm}
    {MASTER OF SCIENCE\par}
    {in\par}
    {COMPUTER ENGINEERING\par}
    {UNIVERSITY OF PUERTO RICO
    \\MAYAG\"UEZ}{  CAMPUS\par}
    {2019}
    \vspace{.1cm}
    \begin{flushleft}
      Approved by:\\
      \vspace{1cm}
    \end{flushleft}
    \begin{tabular}{l c c c c c r}
     
      \rule{2.5in}{.1pt} & & & &  &\rule{2in}{.1pt}\\
      Manuel Rodr\'iguez Mart\'inez, Ph.D. & & & &  &Date\\
      President, Graduate Committee\\
      \vspace{.3cm}\\
      \rule{2.5in}{.1pt}& & & &  &\rule{2in}{.1pt}\\
      Wilson Rivera Gallego, Ph.D. & & & &  &Date\\
      Member, Graduate Committee\\
      \vspace{.3cm}\\
      \rule{2.5in}{.1pt} & & & &  &\rule{2in}{.1pt}\\
      Bienvenido J Velez Rivera, Ph.D. & & & &  &Date\\
      Member, Graduate Committee\\
      \vspace{.45cm}\\
      \rule{2.5in}{.1pt} & & & &  &\rule{2in}{.1pt}\\
      Graduate School & & & &  &Date\\
      Graduate School Representative\\
      \vspace{.45cm}\\
      \rule{2.5in}{.1pt} & & & &  &\rule{2in}{.1pt}\\
      Chairperson, Ph.D. & & & &  &Date\\
      Department Chairperson \\
    \end{tabular}
\end{titlepage}

\setcounter{page}{2}

\addcontentsline{toc}{chapter}{Abstract}
\begin{center}
\doublespacing
Abstract of Thesis Presented to the Graduate School\\
of the University of Puerto Rico in Partial Fulfillment of the\\
Requirements for the Degree of Master of Science in Computer Engineering\\
\vspace{.1cm}
\large\textbf {Find Similar Tweets Within Health Related Topics}
\end{center}
\doublespacing
here abstract of thesis.\\
Here abstract of thesis.\\	
Here abstract of thesis.\\
\par
\clearpage

\addcontentsline{toc}{chapter}{Abstract (Spanish)}
\begin{center}
Resumen de tesis presentada a la Escuela Graduada\\
de la Universidad de Puerto Rico como requisito parcial de los\\
requerimientos para el grado de Maestr\'ia en Ciencias en Ingenier\'ia de Computadoras\\

\vspace{.1cm}
\large\textbf {Encontrar tweets similares en temas relacionados con la salud}
\end{center}
\doublespacing
Aqu\'i resumen de tesis.\\
Aqu\'i resumen de tesis.\\
Aqu\'i resumen de tesis.\\
\par
\clearpage

\vspace*{\fill}
\begin{table}[H]
	\centering
	\begin{adjustbox}{max width=\textwidth }
		\begin{tabular}{c}
			Copyright \textcopyright\hspace{0.15cm}2019 \\
			\textit{by}\\
			\textit{Danny Gilberto Villanueva Vega}\\
		\end{tabular}
	\end{adjustbox} 
\end{table}
\vfill
\clearpage

\vspace*{\fill}
\begin{center}
	\textit{DEDICATION}\\
	\vspace{2cm}
	\textit{To my Mom, Carin Vega P\'erez. To my sister, Emyli S Rodriguez Vega.}
\end{center}
\vfill
\clearpage

\vspace*{\fill}
\addcontentsline{toc}{chapter}{Acknowledgment}

\begin{center}
	\Large \textbf{Acknowledgments}
\end{center}
to thank family\\
\\
to thank advisor\\

This research is supported by the \ac{US} National Library of Medicine of the \ac{NIH} under award number R15LM012275. The content is solely the responsibility of the authors and does not necessarily represent the official views of the \ac{NIH}. Some results presented in this thesis were obtained using the Chameleon Cloud supported by the \ac{NSF}.
\vfill
\doublespacing

\tableofcontents{}

\newpage
\printacronyms[include-classes=abbrev,name=List of Abbreviations]
\newpage
\listoffigures{}
\newpage
\listoftables{}

\newpage
\titlespacing{\chapter}{0pt}{*1}{*1}

\fancyhf{}
\fancyhead[R]{\thepage} 
\renewcommand{\figurename}{Fig}
\pagenumbering{arabic}
\onehalfspacing
\chapter{Introduction}\label{Chapter 1}
\doublespacing

\section{Motivation}
Social networks have become a very important means to share ideas, discuss news, and opinions on many topics.  They also provide real-time information on sales, marketing, politics, natural disasters, and crisis situations, among others. These networks include Facebook, Twitter, WhatsApp, and Instagram, to name a few. 

In this work, we shall focus our efforts on the Twitter social network. This network provides a mechanism for people to express their views using short messages (i.e., 280 characters)
called {\em tweets}. 
Users of this network can find each other messages without the need of becoming ``friends'', as it happens in other networks. The analysis of these tweets can enable us to understand the current situation regarding certain topics, for example, discussions related to medical topics (e.g., ``flu'').
Using the tweets, users can monitor and find patterns that give information about some type of disease being discussed in the social network. In addition, it is possible to detect the position,
``mood'', or sentiment of the people around some topic.

For the analysis of all this available information it is necessary to group or categorize the text along similarities in structure and/or meaning. However, this is a challenging task, due to the complexity/ambiguity introduced by  spelling errors or the use of informal language (``slang'').  In the case of tweets, the small size of the message often makes it difficult to analyze without the context provided by previous messages or user interactions. Making use of the data stored in the  \ac{THS} System from the \ac{UPRM} , as one of our sources, it is possible to process all the information more easily and quickly, and use it to analyze and process the data using  \ac{ML} algorithms, a popular branch of \ac{AI} systems.

The view of the world has changed with the recent progress and ubiquity   of \ac{AI} systems in our lives. We can say that we now live in a new world surrounded by \ac{ML} (e.g. Amazon Alexa, writing correctors). Companies like Google, Amazon, Netflix and others are using \ac{AI} algorithms to obtain value and insight of large amount of data that  otherwise would be  impossible to analyze. In particular, search and mining of text data from social media, blogs, and and other sources provided valuable insight to companies about the opinions of customers. 
%	 The value of the information has been ever the key for the growth of companies; therefore, text analysis is a rough task aim to extract value %information to use in business decisions, however this is challenging job due to complexity of \ac{NLP} a field of \ac{ML}  focuses on analyzing the %human language.

In the health care domain, searching text in social medias, blogs, newspapers can provide clues about the diseases that are being talked about 
by citizens of a region. For example, 

The detection of similarity in texts in their meaning of semantics content is a topic present in many researches because the need to obtain valuable and reliable information from the amount of available data over internet like, communication services (e.g. “Twitter”), feedback user, system log files, customer reviews to mention a few; and the data present in the same company about employees, clients and others. 

In this project, we investigate and implement text similarity algorithms in such a way that we can: 1) know if they are related or not with a disease, 2) group similar tweets to those that we have already captured, analyzed or stored and, 3) find similarity index between tweets using different learning algorithms. We based our work on, semantic similarity approaches and text similarity measures using \ac{DL} algorithms to deliver reliable information to the end user about health-related topics.


\section{Objectives}
REVISE in light of new work
\begin{itemize}[nolistsep]
	\item \textbf{Collect and filter the data file: } We select necessary tweets and filter all them which are related to health disease thereby, we use a clean data as input to train the algorithms to be implemented. Tweets were collected using \ac{THS} System at \ac{UPRM}. In this way, it is convenient to describe the steps through the process of data selection until deliver of final cleaned up inputs. Part of inputs is labeled data by hand; thus, it is necessary a group of people to classify a measure of similarity between tweets that will be used in training sample.
	\item \textbf{Investigate and implements \ac{DL} algorithms for text similarity measures} It is necessary investigate the state-of-the-art methods and techniques related to text analysis, and then build a robust architecture using \ac{DL} algorithms like, \ac{CNN} and \ac{RNN} on \ac{NLP} approaches,  focusing in text similarity measures. The output of our trained models will be one of two of next options a) there is an acceptable similarity measure between the pair of tweets and, b) no exist enough similarity between the pair of tweets.
	\item \textbf{Test algorithms in a Big Data environment: } The algorithms will be tested to measure the performance and accuracy of results in \ac{THS} cluster located in the Electrical and Computer Department at \ac{UPRM}. Because \ac{DL} algorithms consume a lot of resources, we also use virtual environments as Chameleon Cloud Platforms to test algorithms with better Graphic Processing Unit (GPU) resources than physical machines.
\end{itemize}

\section{Contributions}
\begin{itemize}[nolistsep]
	\item \textbf{Use social networks to get valuable information about health topics:} All data present on internet through social networks, entertainment apps and others, can be used in health-related research. In our case twitter is a source of amount of data of different topics that can be transform in important information relevant for medical issues. Here is showed how we can use data of medical conditions to build models able to compute the similarity in tweets therefore these could be used in future to support on medical applications. 
	\item \textbf{Present \ac{DL} models for text similarity analysis:} \ac{DL} models are very powerful for analysis of data like images, sound, and text. In this project, we try to use these excellent tools of \ac{ML} to figure out a better text similarity model.
	\item \textbf{Employ Supervised Learning in text similarity tasks:} Many studies about sentence representation (``encoding models'') is based in unsupervised learning because there is not enough labeled data about a specific topic to train a model like in our case, with data diseases related. We show the trained models with labeled data in sentence similarity has enough performance to be widely adopted in others \ac{NLP} tasks.
	\item \textbf{Use different measure of similarity:} In this project we used three methods to calculate similarity text. Most known is cosine similarity, also Frobenius Distance and our own distance measure called Triangular UL Distance based in part of linear algebra, in a special kind of square matrix called triangular matrix.
	\item \textbf{Describe the evaluation of similarity models: } We built models with \ac{CNN}, \ac{RNN} and merged approaches to get better results. They were tested with different setting to find the best model possible, we used the next metrics F1-Score, Precision and Recall.
\end{itemize}

\section{Outline}
The outline of this thesis is as follows. Chapter 2 contains the literature review about concept of \ac{ML} and \ac{DL} to contextualize the presented solution. In this chapter also in describe topics of \ac{NLP} and text similarity methods. The problem description and the methodology followed to get the similarity models is described in Chapter 3. In Chapter 4, explains the \ac{ML} architecture and describe the different \ac{DL} models built using \ac{CNN} and \ac{RNN} approaches. Chapter 5 shows the result of performance and accuracy of all models described on above chapter. Finally, Chapter 6 shows the conclusions and future work to follow.

\onehalfspacing

\chapter{Literature Review} \label{chapter 2}
\section{Introduction}
%Advance of technology has been many changes in the world and our life is now surrounding for \ac{ML} algorithms hence, companies of all sizes are %following the large business' success using \ac{AI} to draw insight that can be used to take better decisions.

The field of \ac{AI} seeks to understand how humans think (``intelligence''), and how use that knowledge to build programs that show intelligence. 
Today \ac{AI} is a very wide field, making it difficult to give a simple answer to what its an AI application. But we can mention a few applications that exist today:  autonomous robotic vehicles, speech recognition, autonomous planning and scheduling, game playing, spam fighting, logistics planning, robotics in 
automation, and machine translation. These applications combine efforts in computer science, engineering, mathematics, and cognitive sciences \cite{Russell2010}.This applications need process a lot of data, hence it is necessary automate the process of data acquisition and analysis. 
Big data analytics is used to for the purpose of data management in this context.  Machine Learning (\ac{ML})  (a subfield of \ac{AI}) provide the tools  that automate the process of learning  how to extract patterns from the raw data to get insights \cite{Kelleher2015}.

Artificial Neural Networks and Deep Learning are very popular methods in \ac{ML}. Neural Networks are simply a collection of connected computational 
units that represent abstractly the human brain (“neurons”) and  aim to achieve learning on a specific task \cite{Russell2010}. These neurons are organized and interconnected into layers. Deep learning is the development of complex neural networks that contain many layers of neurons. These networks are computational expensive to train and require relatively high amounts of training data. But, they can rival human expertise at various taks, such as image classification or language translation.  

In this project we are working on similarity tasks related with text messages from Twitter. This problem bear similarity with t many applications of similarity for handwritten digits recognition, image similarity detection from text, and image content search  in  web searchers (e.g. Google, Bing).  Neural Network techniques can be successfully applied for these problems, but to achieve better results and scale to large applications we need used techniques specialized on certain domains, for example in case of \ac{NLP} (``text analysis'') we need methods to process sequential data, like {RNN}, that is a{DL} technique part of {ML} field \cite{Goodfellow2016}.  In the rest of this chapter, we go over background information related with machine learning that is necessary to understand the context and contributions of our work.

\section{Machine Learning}

Machine learning, \ac{ML}, also known as automated learning, is associated with the concept of making computers learn some tasks without actually 
programming the computer to do so. Typically, \ac{ML} involves a training phase, where an algorithm is implemented in software and then presented
with many examples of the task to be learned. The training phase produces a {\em model} which is the trained, ``intelligent" software module that automates some tasks. This model is then
validated in the second phase, called {\em validation phase}, where unseen examples are given to the model and we evaluate its performance. 
Typically models are evaluated based on either accuracy of prediction (i.e., classification tasks)  or on the mean square error incurred when approximating some quantity (i.e., regression tasks). If multiple, competing  models are trained and validated, then the final selection is done in a third phase called the 
{\em test phase}, where additional examples are given to the competing models to pick the best one amongst them. 
Notice that this learning is composed of  input data that represent previous experience, and the model will learn to produce an output based on such input.  \cite{Shai2014}. 
This learning process is focused on gaining knowledge, understanding, experience and skills \cite{Nilsson1998} in such a manner its performance improves significantly over time.

%Today we look that \ac{ML} is in many practical applications that we use in frequently in our daily life like: movie recommendation, text translation, speech recognition \cite{Goodfellow2016}, robotic vehicles, autonomous planning and scheduling, diagnosing diseases \cite{Russell2010}. In essence, we are speaking about \ac{ML} when an artificial intelligence system has the ability or capability to get knowledge or find patterns from raw data \cite{Goodfellow2016}. \ac{ML} study data to detect patterns to be able to categorize, predict, identify unknown pattern and detect anomalies or unknown patterns, because Big Data we now have the advantage to process an amount of data, and using {ML} algorithms we can identify new opportunities to solve complex problems like self-driving cars, fraud detection, virtual assistants, resource optimization and more applications \cite{Nevala2017}.

\ac{ML} is a very wide field, for this reason it has branched into several sub-fields related to distinct tasks \cite{Shai2014} and approaches to solve problems. For the study and application of these algorithms,  there are many ways to organize the learning paradigms. The best known are supervised learning and unsupervised learning. This distinction is by what kind of experience they are allowed to have during the training process \cite{Goodfellow2016}. Others kinds of {ML} are semi-supervised learning and reinforcement learning.


\subsection{Supervised Learning}
In Supervised Learning, a model is presented  with pairs of input and output items. Often, the input data items in the pairs are called {\em features}.
Likewise, the output data items in the pairs are often called  {\em labels}. 
%Supervised Learning is based in an set of input-output pairs, 
During training, the model automatically learns   the relationship between the input features and the labels. In 
essence, the model is learning a  function that maps from the input data to the output data \cite{Russell2010} \cite{ Kelleher2015}. This kind of learning is only possible if we know the output data \cite{Nilsson1998}, and if we have the enough examples to train the {ML} algorithm. A good rule of thumb 
by Professor Andrew Ng, from Stanford University, is to have at least 10,000 examples to train your models.


\begin{figure}[H]	
	\centering	
	\includegraphics[width=150mm, scale = 1]{images/1_Supervised.png}
	\caption{Supervised Learning Workflow.}
	\label{figure:Supervised_Learning}
\end{figure}

Figure \ref{figure:Supervised_Learning} shows the process of a supervised learning model. The input of the model is data that contains each element with its respective label, next the model extracts the most relevant features of the data, to find the relation between input and output, and construct a logical pattern. Finally, once the model is trained,  it  can predict new outputs from  new, unseen, unlabeled data items.

The nature of the labels is dependent upon the type of learning problem being tackled.
If the supervised tasks involves classification, then the label represents 
the class or grouping to which the input in the pair belongs. An example classification task will be determining if an image represents a cat or a dog based on the pixels in the image. If the tasks involves regression, then the label represents a quantity associated with the input features. For example, we 
could predict expected battery life of a lithium battery  as a function of daily depth of discharge.  

%More abstractly, Supervised Learning describe a scenery where the training examples is a set of data that contains the necessary information to identify and associate it to the output value. This information is not available in the test data to which the learned model is being applied. The aim is that the acquired expertise will can predict the expected output \cite{Shai2014}.

%It is called ``supervised'' because the environment provides an extra information, commonly known as labels, the model is trained with input data and target data \cite{Shai2014}. The target value (``label'') is provided by a supervisor who teach to the system what to do \cite{Goodfellow2016} , providing the correct output (``desired output'') as a feedback for reduce error.

%Supervised learning identify correlation and a logical pattern in data from the from state A to state B, after these pattern are learnt, we can transfer learning to solve similar problems \cite{Nevala2017} \cite{Cer2018}. 

Common techniques used for supervised learning are logistic regression, decision trees, neural networks, and support vector machines.
Supervised learning is used in applications such as risk assessment, digital assitants, speech recognition, language translation, 
image processing, fraud detection, and  customer segmentation \cite{Nevala2017}.

\subsection{Unsupervised Learning}
In unsupervised learning, there are no labels in the data. Instead, the model tries to uncover the underlying structure in the data by 
finding correlations and identifying patterns parsing the available data \cite{Nevala2017}. Unsupervised learning is often used as 
an initial phase during data analysis to figure out the structure of the data. 
%
%%Today Big Data is a great tool that makes possible analyze a great amount of information, faster and easier.  Raw data in many cases is difficult to analyze and many cases we do not have an answer key for our trainable data. In this cases we can use unsupervised learning like an alternative, therefore we can determine 
%
%This kind of algorithms learns patterns without an explicit feedback (``label'') \cite{Russell2010}, we only have a training data without target values for them \cite{Nilsson1998}. 
There is no separation of training data and test data \cite{Shai2014}; all the data is the input for the algorithm. This method is similar to human behavior: when we observe the world, usually we do inferences and group things based on our interaction with the environment.
This type of learning is refined by exposing the model to  a lot of observations \cite{Nevala2017}.

The most common technique for this learning modality is called {\em clustering}, \cite{Russell2010}, which separates the data into meaningful categories called {\em clusters}. Each cluster is made up of data items that have similar characteristics called clusters \cite{Nilsson1998} \cite{Goodfellow2016}. This similarity is measured by using a function that computes a similarity metric, often a variation of a distance function. In practice the items are represented as vectors 
$v_1,  v_2, ..., v_k$ and the metrics measures how close are each other in their vector space. Other unsupervised techniques are nearest neighbor mapping, singular value decomposition,  self-organizing maps, Hidden Markov Models, and anomaly detection, to name a few. Their applications are related to market basket analysis, anomaly/intrusion detection, text classification, identifying similarities amongst data elements, sentiment analysis, and so on \cite{Nevala2017} \cite{Halibas2018}.

\begin{figure}[H]	
	\centering
	\includegraphics[width=150mm, scale = 1]{images/2-Unsupervised_Learning.png}	
	\caption{Unsupervised Learning Workflow.}	
	\label{figure:Unsupervised_Learning}
\end{figure}

In Figure \ref{figure:Unsupervised_Learning} we show a workflow for unsupervised learning. In this case, we show clustering of documents based on content. First, the text and other features in documents are converted into a vector representation (i.e., one-hot vectors), then a clustering algorithm (e.g., k-means clustering) is used to assign each vector to a cluster. The number of cluster is pre-defined at the start of the learning process.
Once trained, the model is able to take new data items and cluster them.

\section{Neural Networks}
An artificial neural network is a very simplified abstract model of a biological brain, an interconnection of processing units with capability to learn patterns to generalize and associate data. A significant aspect adopted from biological brain is the ``learning capability" from the experience and transfer knowledge to find reasonable solutions to similar tasks \cite{ Gurney2004} \cite{ Kriesel2005}.

When we speak about a neural network, it can be something as simple as a network with a single node to more complicated a collection of nodes stacked in layers\cite{ Kriesel2005}. The structure of  the neural network basically consists of:  1) a set of linked nodes  associated with a numeric weight that represent the strength of connection between them, 2) an input function for each node that computes the weighted sum of all inputs, and 3) an {\em activation function} to control the neuron ``trigger" behavior and get the desired output \cite{ Kriesel2005} \cite{Russell2010}. 

\begin{figure}[H]	
	\centering
	\includegraphics[width=160mm, scale = 1]{images/3_neuron.png}	
	\caption{A Simple Mathematical Neuron Representation as depicted in \cite{Russell2010}.}	
	\label{figure:Mathematical_Neuron}
\end{figure}

Figure \ref{figure:Mathematical_Neuron} show the basic structure of a simple mathematical neuron, where output is result of applying an activation function to the weighted sum of inputs. The input is a collection values of $a_1, a_2, ..., a_i$.The output is a value $a_j$. Each of the inputs are multiplied by a weight $w_{i,j}$ which is a real number. The weights are important to minimize the cost of activation function and they are updated when the model is trained, through an optimization process. Often an additional term $a_0=1$ and weight $w_{0,j}=1$ are added as the bias term in the equation. The following equation represent the output after applying an activation function in a neuron showed in Figure \ref{figure:Mathematical_Neuron}.

\begin{equation}
a_j=g\Biggl(\sum_{i=0}^{n} w_{ij}\; a_i\Biggr),
\end{equation}
The activation function $g$ provides a way to add a non-linear relationship between the inputs and output. Some the functions used as activation function are the sigmoid function, the hyperbolic tangent function, and the rectified linear unit function (RELU). \cite{Goodfellow2016}. 

An alternative formulation, based on linear algebra, represents the examples as a collection of vectors $X_1, X_2, ..., X_k$. These vectors 
are stacked horizontally on to form a matrix $X$. Next we have the labels $Y_1, Y_2, ..., Y_k$ which are also stacked in a matrix $Y$. 
Then the equation for the neural network becomes: 
\[
Z = WX + b
\]
\[
Y = g(Z)
\]
In this formulation, $WX$ is the matrix multiplication between the examples and the weights, and $b$ is the bias vector. $Z$ represents the 
result of theses operations and become the input to the activation function $g$ which finally yields $Y$.

\subsection{Sigmoid function}
The {\em sigmoid function} is used to compute binary activation values in the range [0, 1]. The function is given by the following equation:
\[
\sigma(Z) = \frac{1}{1 + e^{-Z}}
\] 
The sigmoid function is interpreted as computing a probability, and is used in binary classification. Typically, an input on which the sigmoid  gives a values greater or equal to 0.5 is classified to the 1 class, whereas a values less than 0.5 is sent to the 0 class. The sigmoid function is typically used 
in the final node of a neural network. 

\subsection{Hyperbolic tangent function}
The {\em hyperbolic tangent function} is used to compute an activation in the range [-1, 1]. and is used for activation in intermediate nodes 
in a network. The function is given by the following equation:
\[
tanh(Z) = \frac{e^Z - e^{-Z}}{e^Z + e^{-Z}}
\]
Typically, $tanh$  is used for activation in intermediate nodes in a network.

\subsection{Rectified linear unit function}
The {\em rectified linear unit  function} (Relu) is used to compute an activation in the range $[0, z]$, where $z$ is a real number. 
The function is given by the following equation:
\[
g(Z) = max(0, Z)
\]
Typically, $Relu$  is used for activation in intermediate nodes in a network. This function provides better convergence in deep neural networks. 

\subsection{Softmax function}
The {\em softmax function} is used to map a vector K with n dimension into n probability values. The formula is:
\[
s(K_i) = \frac{e^{K_i}}{\sum_{j=1}^{n}e^{K_j}}
\]
Softmax is always used in the final node of a neural network used for multi-class classification. The value $s(K_i)$ represents the probability that 
the input example belongs to class $i$. Typically, the max value of all $K_i$ is used to select the class $i$ with highest probability. 

\subsection {Connecting Layers}
To form a network, we  need to connect individual neurons as nodes in a network graph. There are many ways to build the network, but feed-forward networks are the most common ones.  Figure \ref{figure:feed-forward-network}  shows the topology of a feed-forward network. In this organization, the components are clearly separated: an input layer, an output layer and one or more hidden layers (also called processing layers). The input data is fed at
the input layer, and the output label is  generated by the output layer. For a given neuron at layer $l$, its output is fed as input to neurons in the next layer $l + 1$. 

%All connections are directed to the following layer and the internal states are just the weights themselves. The second network design the neurons have extra connections adding to a classic feed-forward network, they can be a direct recurrence when the neurons are connected to themselves,  indirect recurrence is a connection to previous layers and a lateral recurrence exist when neurons have connections with another ones at the same layer. These connections influence the neurons itself and their influence depends of the kind of recurrent design. This mean that the network is a dynamic system, the fact that this network feeds its outputs back in its own inputs permit a short-term memory, a model more seem to a brain and by the way, more difficult to understand and build \cite{Kriesel2005} \cite{Russell2010}. 

\def\layersep{5cm}
\begin {figure}[H]
\centering
\resizebox {\textwidth} {!} {
	\begin{tikzpicture}[shorten >=1pt,->,draw=black!100, node distance=\layersep]
	\tikzstyle{every pin edge}=[<-,shorten <=1pt]
	\tikzstyle{neuron}=[circle, fill=red!100, minimum size=20pt,inner sep=0pt]
	\tikzstyle{input neuron}=[neuron, fill=black!100];
	\tikzstyle{output neuron}=[neuron, fill=black!100];
	\tikzstyle{hidden neuron}=[neuron, fill=black!100];
	\tikzstyle{hidden1 neuron}=[neuron, fill=black!100];
	\tikzstyle{annot} = [text width=4em, text centered]
	
	% Draw the input layer nodes
	\foreach \name / \y in {1,...,3}
	\node[input neuron] (I-\name) at (0,-2*+\y) {};
	
	% Draw the hidden layer nodes
	\foreach \name / \y in {1,...,8}
	\path[yshift=0.5cm]
	node[hidden neuron] (Hb-\name) at (\layersep,-\y cm) {};
	
	\foreach \name / \y in {1,...,8}
	\path[yshift=0.5cm]
	node[hidden neuron] (H-\name) at (2*\layersep,-\y cm) {};
	
	\foreach \name / \y in {1,...,8}
	\path[yshift=0.5cm]
	node[hidden neuron] (Ha-\name) at (3*\layersep,-\y cm) {};
	
	
	% Draw the output layer node
	%\node[output neuron,pin={[pin edge={->}]right:Output}, right of=H-3] %(O) {};
	\foreach \name / \y in {1,...,2}
	\path[yshift=0.5cm]
	node[hidden neuron] (o-\name) at (4*\layersep,-3*\y cm) {};
	
	% Connect every node in the input layer with every node in the
	% hidden layer.
	\foreach \source in {1,...,3}
	\foreach \dest in {1,...,8}
	\path (I-\source) edge (Hb-\dest);
	
	\foreach \source in {1,...,8}
	\foreach \dest in {1,...,8}
	\path (Hb-\source) edge (H-\dest);
	
	\foreach \source in {1,...,8}
	\foreach \dest in {1,...,8}
	\path (H-\source) edge (Ha-\dest);
	
	% Connect every node in the hidden layer with the output layer
	\foreach \source in {1,...,8}
	\foreach \dest in {1,...,2}
	\path (Ha-\source) edge (o-\dest);
	
	% Annotate the layers
	\node[annot,above of=H-1, node distance=1.5cm] (hl) {Hidden layers};
	\node[annot,above of=I-1, node distance=3.2cm] (il) {Input layer};
	\node[annot,above of=o-1, node distance=3.2cm] {Output layer};
	\end{tikzpicture}
}
\caption{Feed Forward Network}
\label{figure:feed-forward-network}
\end{figure}


\subsection{Parameters and Hyperparameters}
The {\em parameters} of a neural network model are the weights $w_{i,j}$ in each layer. Often, the notation $W^{[l]}$ is used to denote the weight matrix $W$ used at layer $l$. If we have a network with $n$ layers, then we will have $n$ weight matrices $W^{[1]}, W^{[2]}, ..., W^{[l]}, ...W^{[n]}$.  Again, the values in these matrices are called the {\em parameters} of the model. The \ac{ML} training process
involves the use an optimization algorithm that finds the right set of parameters $W^{[l]}$ for each layer $l$. These parameters are chosen as to minimize the error between the predicted label $\hat{Y}^{(i)}$ and the actual label $Y^{(i)}$ for each input example $X^{(i)}$. The most popular algorithm in used is called Gradient Descent, and has many variations such as ADAM, RMSprop, and ADADELTA \cite{Goodfellow2016}. 

In most instances, there might be multiple models to use on a given problem. In addition, multiple optimization algorithms can be tried to train a particular model. These different alternatives have impact on the training process and the parameters of the models. They are often called {\em hyperparameters} because they are not really parameters of the model, but are variables that affect training. Example hyper parameters include: a) network architecture, 
b) optimization algorithms, c) learning rate for the optimization process, and d) methods to prevent overfitting. {\em Overfitting} in a situation where the model 
works well on the training data but fails to generalize to the validation data set. 

\subsection{Data Splitting} 
The recommended procedure to find the right model involves splitting the sample data into three parts:
\begin{enumerate}
	\item {\bf Training Dataset} - this is the data used to train the network and find the parameters of the model. It is recommended to use 60\% of the data 
	for this task. 
	
	\item {\bf Validation  Dataset} -  this data is used to validate the performance of the model for a given combination of hyperparameters used for training. It is recommended to use 20\% of the data for this task. 
	
	
	\item {\bf Test Dataset} - this data is used to select the best model from a collection of models that have been trained and validated. 
	It is recommended to use 20\% of the data for this task. 
\end{enumerate} 

\section{Deep Learning with Neural Networks}

%The artificial intelligence usually solves problems intellectually challenge for the humans relatively easy, but the task the human performs easy or solve intuitively are very difficult for the computers. The problems like speech recognition or applications to find faces in images are challenge task. For these problems we must allow computers gather knowledge from the experience, avoiding specify all the knowledge that the system needs. It means build a complicated task using simpler concepts \cite{Goodfellow2016}.
%
%This approach is a \ac{ML} modern and advanced technique, the base in many advances of  \ac{AI} fields, like Natural Language Processing, Speech Recognition, Computer Vision, Biomedical Applications and so on; using the most sophisticated neural networks. 

Traditional neutral networks only contain a few layers, for example: one input layer, one to three hidden layers, and a final output layer. 
Modern approaches,
however, add dozens of hidden layers with tens or even hundreds of neurons in each layer. These approaches result in models that
form complex and deep networks graphs, much more powerful  than a regular neural network. For that reason,  we know this approach as ``Deep Learning" (DL) \cite{Chandra2017} \cite{Goodfellow2016} \cite{Nevala2017}. Improvements in hardware, particularly Graphical Processing Units (GPU) units for massive parallel computing of neural networks have enable this ``deep approach". Without this technology, the computational costs to find the parameters in the deep networks would made them difficult, if not impossible, to train because they contain millions of parameters. In contrast, small 
neural networks with just a couple of layers have just a few thousand parameters and are easier to train. 

The most typical example of \ac{DL} approach is based on the feed-forward deep network. It consists in an input layer, that contains the input data for the model, next we have the hidden layers, a variable number of neurons and layers, and finally an output layer as is showed in the Figure \ref{figure:feed-forward-network}. By increasing the depth (number of hidden layers) on the network, it becomes possible to learn more complicated relationships. 
Other important methods for  \ac{DL}, include the convolutional neutral network (\ac{CNN}) used  when is working with images,  and the 
recurrent neural network (\ac{RNN})  use to process sequence data. 

\subsection{Fully Connected Networks}

These networks are the traditional fully connected feed-forward networks with many layers, as shown in Figure \ref{figure:feed-forward-network}.
Recall that for each node at layer $l$, its output is connected as input to all nodes in the next layer $l+1$.

% it means all nodes from a layer relate to all nodes of following layer and same way for the other layers. This network is a little similar to \ac{CNN} but could be computationally expensive. With modern hardware and specially using \ac{GPU} is possible achieve tasks successfully. Today we know that is not necessary to train a fully connected deep architectures due the resources that needs, but they were the first to be succeed \cite{Goodfellow2016}.

\subsection{Convolutional Neural Networks}
Convolutional neural networks (\ac{CNN}) are applied to problems with multi-dimensional data that can be represented in tensor forms (e.g, 2-D matrices, or 3-D 
cubes). For simplicity, the input of a \ac{CNN} is called an image since they are extensively used for image processing.  
The key idea in these networks is to scan the input image and apply the {\em convolution} operation between portions of the image and one {\em filter}. 
A filter is a smaller matrix $W$ which contains the parameters for the model. The operation is expressed as: 
\[
Z = W*X + b
\]
where $X$ is the image, $W$ is the filter, $b$ is the bias vector, and $*$ is the convolution operation. Like in the case of fully-connected layers, an activation function $g$ can be applied to  the vector $Z$. In convolutional layers, Relu and $tanh$ are often used as activation functions. 

Each layer in a \ac{CNN} is focused in applying a convolution operation. 
There might be more than one filter in each layer, and there can be additional ``pooling layers" used to sub-sample the image and reduce its size. Current \ac{DL} approaches stack multiple convolutional layers in a row and then flatten the image to a one dimensional array. 
This array is then feed into a sequence 
of fully connected layers that end with an output layer. 

%This kind of networks is applied to problems with a grid-like topology, a clear example of this type of data are the images, other are the regular time intervals. \ac{CNN} formally is composed with a mathematical operation called convolution in at least one layer, instead of a general matrix operation \cite{Goodfellow2016}. This kind of algorithm is very used in 
CNNs are extensively used in image processing and pattern recognition, as a powerful visual model, to extract features in a hierarchy of concepts. \cite{Long2015}. Historically this type of network was some the firsts to solve important commercial problems \cite{Goodfellow2016}, like the handwritten zip code number recognition.

\subsection{Recurrent Neural Networks}
Recurrent Neural Networks (\ac{RNN}) are networks for processing sequential data. Examples include speech recognition, language translation, and financial time series data. \ac{RNN} are very powerful because they  efficiently store information about the past. The basic idea in a RNN is to have 
a layer of neurons that process an input data item and produces an output based on the input and a previous value from previous step. Thus, the network forms a feedback loop where the output in one time step $t$ is used in combination with the input at time step $t+1$ to produce the output for step $t+1$. In essence, the \ac{RNN} has a memory of previous events when  it produces the next output value. Like in the case of \ac{CNN}, the 
final layers of the RNN are fully-connected layers used to produce the output of the model.

\ac{RNN} are specialized to process a sequence of values \cite{Goodfellow2016}, mapping all the previous inputs to the final output. This allows the memory of old inputs to persist and influence in the next network output \cite{Graves2017}. \ac{RNN} have also been used in image classification 
\cite{Chandra2017}. Other applications are found in the text processing: sentiment classification, topic classification, summarization and machine translation.

%Train a \ac{RNN} for task that need information over the time is not easy, because a simple network do not maintain contextual information distant from the current step of processing. To do these kinds of task is necessary implement network more complex for maintain information as needed for future steps and remove which are not necessary \cite{Jurafsky2018}. The network more known with this type of design are \ac{LSTM} and \ac{GRU} that are described following.


\subsubsection{RNN based on Long Short-Term Memory (LSTM) }

This type of network manages the contextual information with the ability of ``forgetting" the information that is not useful, and maintaining the information that has more probability to be used in next processes. Each layer has the ability to forget some previous state and give more weight to new input values. 
%The training task is specialized in achieve a better manage of contextual information to discard it or save it. 
To control the flow of information over the gates of the network, there are  specialized neural units to control how the relative weight of the current
vs the output of the previous step.   \cite{Jurafsky2018}. There is a forget gate that controls when  to  forget previous information,  and another gate to control 
how  much importance does new information has.

\subsubsection{RNN based on Gated Recurrent Unit (GRU) }
This kind of network is an easier way to implement a recurrent network with control of memory, and it was  is meant to be is a special case of a \ac{LSTM}. In a GRU, there is only one gate  that controls both the ability to forget about previous data and give more importance to new input values. 
\cite{Jurafsky2018}.

\section{Natural Language Processing}
Since our goal is to find a way to measure the similarity between text in tweets, we need to briefly discuss the field of natural language processing
(\ac{NLP}). 

Basically, \ac{NLP} is the process to understand, analyze and use the human language by machines or algorithms. 
This includes  written text, voice commands or both. The goal is to allow computers to comprehend natural language, receive 
commands, and produce results in human language. There is a lot of information contained in web pages, video sites, and social networks.   
Almost all of it is
written or recorded in natural language, and that is a major  reason why we want our algorithms to understand human language, in  oder   to acquire 
all this information from written and spoken language. A \ac{ML} model that wants to acquire knowledge needs to comprehend at least partially the  use of human language  (e.g. ambiguity and messy). This undertaking is not trivial because the presence of the colloquialism, figure of speech, abbreviations, emoticons to mention a few \cite{Nevala2017} \cite{Russell2010}. The difference with a computer language (e.g. python or java) is that a natural language cannot be characterize in a set of sentences, because they are very large, ambiguous and constantly changing;  state-of-the art models
models are just an approximation \cite{Russell2010}.

The process of language analysis is closely related to linguistic, traditionally, this process is decomposed in three stages: syntax, semantic and pragmatics. We first analyze the syntax of a text providing an order and a structure, and then  analyze  the text in terms of meaning (semantic), and the last stage is a pragmatic analysis, which relates the sentence with a context. Such separations constitute the basis for structural models from a software point of view, making the analysis more manageable and serving as a starting point. Recently, deep learning solutions have done away with scheme 
and try to understand language straight from the raw data examples. \cite{Indurkhya2010}

\begin{figure}[H]	
	\centering
	\includegraphics[width=150mm, scale = 1]{images/4_nlp.png}	
	\caption{Natural Language Process Stages}	
	\label{figure:NLP_Stages}
\end{figure}

Figure \ref{figure:NLP_Stages} represent the stages in a  traditional \ac{NLP} pipeline. The first step, tokenization,  divides a continuous text  into words or sentences. In English, text is usually separated by whitespaces but other unsegmented languages are more difficult because there is not a visible separation between words (e.g. Chinese, Japanese). The second stage is lexical analysis, which refers to morphological processing, mapping strings to lemma and assigning structure to words and registering their structural properties. The third stage is syntactic analysis where a sentence is process to determine its structural composition, following rules of a formal grammar to finally assign a meaning to the sentence. The syntax tree is the standard to represent a syntactic structure which contains the steps in the derivation from the initial sentence. The fourth stage is semantic analysis, and is related with the context of words, phrases, etc. This stage not only refers to the meaning of a sentence, but also to its relationship with others words or sentences, to infer meaning of the text. The final stage is pragmatic analysis also called natural language generation, and  is the process when a machine can understand natural language as input and produce natural language as output with the intention to communicate something with a correct context  \cite{ Indurkhya2010}.

Applications of \ac{NLP} are based on probability distributions over sequential data in a natural language, we can use generic neural networks for basic tasks, however in complex applications we need specialized techniques to process data, sometimes we regard like a sequence of words, characters or even bytes. The earliest successful language model, a probability distribution over sequential data, was n-gram models (sequences of tokens). It has many possible types: unigrams, bigram, trigram, n-gram. In order to improve statistical efficiency, researchers introduced the notion of word categories or class-based n-grams models, however they are not able to share statistical strength of similar words and their contexts. 

Neural network  models have been able to recognize the similarity not only between words but also in their contexts and how words are distributed with each other. This word representation  representation sometimes is called  distributed representation or word embedding, which means that language symbols are represented as points in a multi-dimensional vector space . In an embedding space, words with similar meaning are close each other \cite{Russell2010} \cite{Goodfellow2016} because they are represented by vectors that close by in the vector space (in the Euclidean distance sense) . The best known word representation are Word2Vec Models \cite{Mikolov2013}, an improved of skip-gram model in terms of quality of vectors and speed training \cite{Mikolov2013} and GloVe model that combines the advantages of global matrix factorization and local context window \cite{Pennington2014}. More recent studies show other sentences encoder like Skip-Thought Vectors, an approach for unsupervised learning that tries to reconstruct passages of text that share semantic properties \cite{Kiros2015}. There is also the  Supervised Learning of Universal Sentence Representations from Natural Language Inference Data, an encoder using pre-trained sentence level embedding with supervised data of Stanford Natural Language Inference datasets demonstrating stronger transfer task than skip-thought vectors \cite{ Conneau2017}. Also, we have the Universal Sentence Encoder, other encoder that uses supervised data to train a sentence embedding model instead a word level embedding, showing a good performance \cite{Cer2018}. 

There are several important applications of \ac{DL} techniques in \ac{NLP} tasks including:

\begin{itemize}[nolistsep]
	\item \textbf{Spelling and Grammar Checking:} The suggestions that appear when we type a text in a smartphone or when we redact a document in an advance text editor. 
	
	\item \textbf{Text Classification:} It is the categorization of a text in a defined set of classes (e.g., hate speech), or the language identification, or the  spam detection in email messages \cite{Russell2010}.
	
	\item \textbf{Information Retrieval:} Search, retrieval, and ranking  of documents responding to a keyword query.
	
	\item \textbf{Summarization:} Automatic generation of  the essential information from a long document into a shorter piece of text.
	
	\item \textbf{Syntactic Analysis:} The analysis of a string of words to isolate subjects, verbs, adverbs, adjectives and others complements (phrase structure) \cite{Russell2010}.
	
	\item \textbf{Machine Translation:} It is the task to automatic translation of a sentence into an equivalent sentence in meaning in another natural language \cite{Goodfellow2016}.
	
	\item \textbf{Speech Recognition:} This task identifies the words spoken by a speaker to determine their meaning. That is difficult because the ambiguity and noisy are a hard challenge \cite{Russell2010}.
	
\end{itemize}

\section{Text Similarity}
{\em Text similarity} refers to how close is a piece of text $T_1$ to another one $T_2$ in terms of meaning and structure.  The first means semantic similarity and the last is called lexical similarity. All works in lexical similarity often are developed to achieve to semantic similarity \cite{Ganesan2015}.  Lexical similarity exists when there is character or word matching (e.g. ``feel'' and ``feet''). On other hand, semantic similarity is based on meaning and context (e.g. ``President of the United States' and ``POTUS") \cite{Pradhan2015}. 
Text similarity is used in many \ac{NLP} problems such as text classification, clustering, information retrieval, topic detection, machine translation, summarization and others \cite{Majumder2016} \cite{Gomaa2013} \cite{Pradhan2015}. It is used heavily in research related with social network analysis \cite{Zhang2015}.  

\subsection{Lexical Similarity}
When we talk about of lexical similarity, it refers to structurally similar words or characters in the evaluation sentences (matching or comparison). 
%Hence in this kind of similarity, the meaning is not known then we can use lexical analysis techniques to achieve better accuracy, for example we can add no-determiner words to expand the scope and provide some of context \cite{Ganesan2015} \cite{Gomaa2013}. This metric could be used on clustering, redundancy removal and information retrieval \cite{Ganesan2015}. 
Lexical similarity could categorize text depending of the granularity used. It can be character level, word level or phrase level \cite{Ganesan2015} \cite{Pradhan2015}. String-based methods to calculate the measure of similarity are categorized as follows:

\begin{itemize}[nolistsep]
	\item \textbf{Character based similarity:} There are many techniques  based on characters. The longest common substring/subsequence is a method that finds substrings and compares them based on a common chain of characters they both share. This is the longest common chain of suffixes for each substring. Another idea is the Levenshtein distance, where the similarity measure is given by the number of operations to transform one string in another one, using insertions, deletion or substitution of adjacent characters. Other method is Jaro distance that emphasize in the number and order of matching common characters, it is used in duplicate detection (record linkage). Finally, the n-gram method  calculates the distance of two sub sequences, dividing its similar n-grams by maximal number of n-grams. \cite{Gomaa2013} \cite{Majumder2016} \cite{Pradhan2015}.
	\item \textbf{Statement/Term based similarity:} A method widely known in text similarity is cosine similarity, where the similarity distance is given by the cosine of the angle between two vectors in an inner product space. These vectors represent the sentences being considered. Euclidean distance also called L2 distance use vectors in a Euclidean space where the distance is the square root of the sum of squared difference between two vectors (sentences). Jaccard similarity coefficient or Jaccard index is used to calculate similarity and diversity in sets, it takes the shared term in the intersection and it is divided by number of all terms of the union. There are other known methods like block distance (Manhattan distance, L1 distance, etc.), Dice's Coefficient, Matching coefficient, overlap coefficient, soft-cosine similarity, centroid based similarity and others\cite{Majumder2016} \cite{Pradhan2015} \cite{Gomaa2013} .
\end{itemize}

\subsection{Semantic Similarity} \label{sem_sim}
Another approach for similarity  is about measures of similarity that  have in mind the meaning and context of sentences  even when they could be syntactically very different. Semantic analysis entails a deeper level of analysis, for example syntactic parsing to get dependency structure in the phrases. Commonly semantic similarity use background information about concepts like WordNet, or Wikipedia,  or a simplify corpora of texts  \cite{Ganesan2015} \cite{ Zhang2015}.
Semantic similarity is frequently used for text summarization, topic analysis, recommendation system, collaborative tagging system and others. This approach could be classified in three big categories based in the use of information: corpus based,  knowledge based and hybrid methods \cite{Gomaa2013} \cite{Zhang2015}. These categories are described below.

\begin{itemize}[nolistsep]
	\item \textbf{Corpus based similarity:} 
	Also called statistical similarity, this approach refers to the use  of information gathered from a large corpus from written o spoken data. Latent semantic analysis represents the text in a matrix where rows are the unique words and columns represent each sentence. This method is constructed based in a corpus of text and a mathematical technique called singular value decomposition to reduce dimensionality maintaining similarity relation between word and text. To calculate the similarity often is used cosine similarity.  Other technique is hyperspace analogue to language, it is a variation of latent semantic analysis, taken a set of words called a ``window'' and is compared with the corpus to calculate co-occurrences in words, forming a matrix representing the strength between related words. Its similarity also can measure with cosine similarity method. Explicit semantic analysis uses the Wikipedia corpus to convert sentences in a tf-idf (term-frequency-inverse document frequency) weighted vector between each word and the documents of corpus. The measure to calculate the distance between vector is cosine similarity. Normalize Google distance is a semantic metric that uses Google search to get the number of hits returned for each term of the text and build a metric to calculate the words with similar meaning between two sentences. Other methods are point wise mutual information, normalized information distance, normalize compression distance  \cite{Ganesan2015} \cite{ Zhang2015} \cite{Gomaa2013} \cite{Majumder2016}.
	
	\item \textbf{Knowledge based similarity:} It is called topological similarity and it is based in semantic networks, these networks are a lexical database grouped in set of semantic synonyms with conceptual and lexical relation that include verbs, nouns, adverbs and adjectives. The most popular semantic networks are Word Net and Natural Language Toolkit \cite{Gomaa2013} \cite{Pradhan2015} \cite{Majumder2016}. There are distinct types of categorization for this kind of similarity measure focused in similarity or relatedness. Some authors divide it in the following types: \textit{Node-based} (also called information content based) similarity that calculate the similarity between two sentences by the amount of information that share each other. One method very used in this approach is Resnik similarity method, it uses only Word Net nouns to get the information content of sentences. Other measures are Lin method, Jiang and Conrath method \cite{Majumder2016} \cite{Zhang2015} \cite{Pradhan2015}. \textit{Edge based} approach counts the edges of graphs (semantic network) between the compared concept nodes where nodes with shorter path are more similar. All edges are weighted considering network density, node depth, type of link and link strength \cite{Majumder2016}  \cite{Zhang2015}. Other approach is \textit{feature based}, it uses the concepts of Word Net as a list of features to calculate the semantic similarity between sentences \cite{Zhang2015}, and finally \textit{gloss based} approach uses glosses of words from a given corpus. One method in this group is, Vector Measure, it forms a co-occurrence matrix with the average of each co-occurrences vector of a gloss/concept to measure the semantic similarity \cite{Pradhan2015}  \cite{Zhang2015}.
	
	\item \textbf{Hybrid similarity measures:} This approach refers to use the best of above two categories and others, like lexical structure and corpus information, combining several metrics into one. For example, semantic text similarity method is combination of syntactic and semantic information \cite{Zhang2015} \cite{Gomaa2013}. Other methods combine more approaches, \cite{Zhu2014} built a supervised random forest regression model that use machine learning methods to combine string features, corpus-based methods, knowledge-based method, syntactic features, 
	and
	multi-level text features.
	
\end{itemize}

\chapter{Problem Formulation} \label{chapter 3}
\section{Description}
Our work is rooted in our \ac{THS} project at UPRM, whose goal is to monitor tweets related with medical conditions, and detect instances where
a diseases is actively being discussed in a given region.
The main problem that we want to tackle in this project is the development of  \ac{DL} models that can evaluate the similarity between
tweets. Specifically, given an example tweet $T_p$ and  collection of tweets $T_1, T_2, ..., T_n$, we want to rank the collection of tweets
from the most similar to the least one to $T_p$. In order to do, {\bf we need to develop a method to measure the similarity between pairs of 
tweets}. Our goal is to use \ac{DL} to achieve this. 

First, we need to create a collection of labeled tweets to help us determine how humans 
view similarity. In this collection, we have rows with triplets $(T_p, T_{h_1}, T_{h_2})$, each element being a tweet. For each row, the first tweet, $T_p$ is the ``example" tweet called the ``premise". Next to it, we have two additional tweets, $T_{h_1}, T_{h_2}$, called the
hypothesis tweets. Users are asked to compare the premise with each of the two hypothesis tweets, and rank which one is more similar to the 
premise. This is done by providing a numeric value on their similarity. The value is in the range [0,4], with 0 being unrelated tweets and 4 being identical in term of meaning. Thus, for example, if the rank between $T_p$ and  $T_{h_1}$ is 1, and the rank between $T_p$ and  $T_{h_2}$ is 3, then 
tweet $T_{h_2}$ is the more similar to $T_p$.

With this labeled data set, we then aim to train a model than can learn to do this ranking and learn how to give a similarity score between 
the example tweet $T_p$ and another tweet $T_i$ that we want to consider. Our approach borrows idead from the work in \cite{Wang2014} which used 
a similar idea to find  similarity between images. 

By using a \ac{DL} model trained to rank tweets based on similarity, users could: 
\begin{itemize}
	\item pick a tweet - the example tweet- that they consider useful for detecting conversations about a given disease 
	\item setup a process to watch the Twitter stream and collect tweets similar  to the example one and visualize these tweets  sorted by 
	relevance of their similarity. The model can be used to compute a similarity score between the example tweets and each tweet read from the stream. Then, sorting can be used to provide a ranked list based on the similarity.
\end{itemize} 
As we described in section \ref{sem_sim},   text similarity measures are very difficult to calculate  because it is hard to find the meaning and context match of the two pieces of text. To manage those issues, we use relevant information to find context similarity and syntactical similarity, like the type of disease mentioned in each tweet,  or if the tweet is related to a disease or not. This contextual information can be used as pre-processing step before feeding 
the tweets to the \ac{DL} model.

% and a metric that describe the level of similarity between tweet texts, that are tagged in a range of one to four, where one is less similar and four is more semantically similar.

\subsection{Examples}
We now provide some samples of tweets to show the complexity of work with data related to semantic measures and contextual similarity.
We obtained these tweets form the actual Twitter stream.

\begin{definition} {\bf : Tweets that are not related  and talk about different diseases.}
	\begin{itemize}[nolistsep]
		\item Measles starts with cold like symptoms that develop about of days after becoming infected this is followed a few days later by the measles rash poster shows us the symptoms to look out for think measles prevention is best a dose of vaccine needed vaccines work.
		\item The Ebola outbreak was a worst nightmare for countries affected interestingly Ebola was not a new disease in fact on managing the disease there is a forty year old knowledge so how did Ebola become such an epidemic.
	\end{itemize}
\end{definition}
In this example, the two tweets talk about diseases. The first one of them describes the symptoms of measles, and recommends vaccines as the method
for  prevention. The other one does a question about the reason why Ebola has expanded if this disease is not new and there is enough knowledge about it.  The content of these two sentences are related with a health condition but they are describing different diseases, that is enough to consider them not similar.

\begin{definition} {\bf : Tweets that are not related  and talk about the same disease}
	\begin{itemize}[nolistsep]
		\item Theory man flu exists as a phenomenon because it is the only time toxic masculinity tells men it is okay to feel vulnerable weak and look to their partners for support.
		\item Yeah, I missed like a day of school on and off flu I could not even eat, and I could not walk without feeling like vomiting or collapsing.
	\end{itemize}
\end{definition}
The first tweet refers to the flu, but it is not about a health condition. It talks about  an assumption about how the flu exists to show the vulnerability of weak  men. The  other one talks about someone that missed a school day because flu and describes  his/her symptoms.


\begin{definition} {\bf : Tweets that are  related  and talk about the same disease}
	\begin{itemize}[nolistsep]
		\item I wish I knew if the twins were going to get the flu like what is the usual window, he is had it symptom wise since Sunday night and it is Tuesday so when would they likely show symptoms ahh.
		\item So I was at the hospital and obviously could not type while I was there since, a I could not type and b I was scared of being caught with lewd rps after I healed bad luck struck again and I was bedridden with the flu all I did was sleep and I was coughing so much my eyes went red.
	\end{itemize}
\end{definition}
In this case the first tweet tells about twins getting flu and describe the intervals of days with possible symptoms. The second is about a person that who was in the hospital and explain that was the reason because she/he could not type, and when he or she was recuperated then got flu again and just goes to sleep. The two sentences are showing a health condition (flu) where the writers are explaining symptoms and what they were doing when they got flu. Therefore, these sentences are related.

\section{Formalization}\label{formalization_problem}
We are now in a position to better formalize the problem at hand. Consider a premise tweet $T_p$ and a collection of tweets 
$T_1, T_2,..., T_n$. Let $sim(T_i, T_j)$ be a function that computes the similarity between  two tweets $T_i$ and $T_j$ as their distance 
in some n-dimensional embedding (vector) space. The smaller the value for $sim()$ the more similar are $T_i$ and $T_j$.   Let $rank(T_i, T_j)$ be a function that assigns a comparison rank to $T_i$ and $T_j$ in the range [0,4]. The closer to the value 4 the more similar are $T_i$ and $T_j$.
We define the following property for $rank(T_i, T_j)$:
\begin{property}{\bf Rank Property}\\ 
Given three tweets $T_p, T_{h_1}, T_{h_2}$, if $rank(T_p, T_{h_1}) > rank(T_p, T_{h_2})$ then $sim(T_p, T_{h_1}) < sim(T_p, T_{h_2})$.
\end{property}
With this property we establish the $rank()$ as an alternative method to measure similarity. The higher the rank  the more similar the tweets. 
Our goal is then train in a model $M$ to compute the function $rank(T_i, T_j)$.

Notice that the $sim()$ function is an unbiased measure of similarity, but it is difficult to find a general similarity metric, as discussed in 
Chapter \ref{chapter 2}. Instead, our $rank()$ function
tries to approximate human ability to make the comparison and establish similarity. 

%The goal in this project is classify if a tweet is similar to another from a stream of tweets $S = {s_!, s_2, … , s_n}$. Our model is a supervised approach, where the target keywords are associated with a disease group $G = {g_1, g_2, …, g_n}$. G is conforming by five diseases \textit{flu, measles, zika, diarrhea and Ebola}.

\subsection{Scope of the Model}
Machine learning models are bound to work on data coming from the same distribution as that used for training. In order to properly scope
our model, we limit the data set to consist of tweets that contain the following medical keywords:
\begin{itemize}[nolistsep]
	\item Flu
	\item Zika
	\item Ebola
	\item Measles
	\item Diarrhea  
\end{itemize}
We collected tweets from the Twitter stream that contained any one of these keywords.

Sometimes, the keywords mentioned above are used in colloquial conversations that are not related with medical 
issues, which can mislead researchers into believing the occurrence of a keyword implies a disease outbreak. 
For that reason, the tweets are then are organized and labelled in three classes:
%The belong to a medical condition is given by a manual classification of tweets into the next three classes:

\begin{itemize}[nolistsep]
	\item \textbf{0}: Tweet does not belong to a medical condition.
	\item  \textbf{1}: Tweet belong to a medical condition.
	\item \textbf{2}: Tweet has an ambiguous meaning, it is difficult to classify 
\end{itemize}
Clearly, in a production setting, we are only interested in working with tweets that belong to class 1. 

\subsection{Use in Production}
Once the model the $M$ is trained we want to use it in production following these steps:

\begin{enumerate}
	\item Listen to the Twitter data stream to build a collection ${\bf T} = \{T_1, T_2, ..., T_n\}$.
	\item Chose a premise tweet $T_p$.
	\item Compute the $rank(T_p, T_i) $ for each tweet $T_i \in {\bf T}$, and store the triplet $(T_p, T_i, rank(T_p, T_i))$.
	\item Sort the triplets $(T_p, T_i, rank(T_p, T_i))$ in decreasing rank order.
\end{enumerate}
With this procedure, the tweets more similar to $T_p$ are sorted from most similar to least similar. 

%The last target is a measure of similarity between a pair of tweets in range of one and four. The complete description of how is taken this measure is given in section \ref{define_similarity}
%
%We apply pre-processing and \ac{NLP} tools over tweets to filter and select whatsoever related to a medical condition, and built triplets taking only tweets with labels 1 or 0. Let us call the result of this process S'. Now we apply a classifier $\hat{y}$ in a triplet $s' \in S'$ to classify the pair of tweets in the triplet that are more similar. After training $\hat{y}$ classifier, it can be used as long data come from the same source. If we change G elements, we must be retraining the algorithm with the new keywords present in G.
%
%The distribution of samples respect to belonging to a medical condition or no, are not uniform, with class 1 being majority. In this case we are dealing with a class imbalance problem \cite{Batista2004} \cite{He2009}. To tackle this problem, we use a penalty technique during training task, a higher penalty is given when occur a wrong classify of a sample that belongs to the minority class.

\section{Data Processing Architecture}

\begin{figure}[H]	
	\centering
	\includegraphics[width=150mm, scale = 1]{images/5_data_processing.png}	
	\caption{Data Processing Architecture}	
	\label{figure:data_processing}
\end{figure}
 Figure \ref{figure:data_processing} depicts the architecture for 
 tweet processing that we shall implement to train and use our models. The architecture is based in the THS system \cite{8029348, 8622504}.
 Tweets are collected in  the \ac{THS} system and stored in a distributed database system  over \ac{HDFS}. Tweets are cleaned, removing links, hashtags and mentions.  
 %In a next stage the text passes through a pre-processing expanding contractions and correcting misspelling.
In the next stage, tweets are filtered to associate with a disease. After detecting if the text is related to a disease, tweets are labeled to know if each tweet is speaking about a disease or if it is a lexical or semantical ambiguity. This is done using the model trained in \cite{8622504}. 
In order to train the similarity model, triplets of tweets are given to human labelers so they can provide a score of similarity between the 
first tweet (premise) and the other two ones (the hypotheses). 
%Other process to prepare the data to training is take an average measure of three labelers about the level of similarity between tweets as part of pre-processing stage.
Once the similarity model is trained, it can be used in production for find the similarity of tweets. 

%The previous stages are the input of the main process to train algorithms and calculate tweets relevance and after to present these tweets to the user sorted by relevance of their similarity as the output of the model.

\chapter{System Architecture and Organization} \label{chapter 4}
\section{THS System Overview} \label{ths_system}

\ac{THS} is a collection of Big Data tools running on a cluster system at \ac{UPRM} to collect, filter and store the tweet data to future uses. We use \ac{ML} tools to process data:  Keras \cite{Keras2019}, TensorFlow \cite{Tensor2019}, Scikit-learn and others. Our current \ac{THS} system consists of 12 nodes in a big data environment, organized as follows: 1 master-node, 1 client-node and 10 data-nodes. The big data tools installed are \ac{HDFS} \cite{Hadoop2019}, \ac{YARN} \cite{Hadoop2019},  Apache Kafka \cite{Kafka2019}, Apache Spark \cite{Spark2019} and Apache Hive \cite{Hive2019}.

The data processing pipeline is shown in the Figure \ref{figure:ths_system} and detailed step by step as follows:

\begin{steps}
\item We subscribe to the Twitter \ac{API} to save raw tweet data, building a real-time data stream using Kafka, a distributed streaming platform \cite{Kafka2019}. The script (``producer.py'') runs on the edge-node (client-node) of our cluster identified as node05.ece.uprm.edu. In this step we filter data by language (e.g. English) and by keywords of target medical disease (e.g. Flu, Measles). The purpose of this script is to keep with  the Twitter stream, collecting data, and 
passing it down the pipeline for its eventual processing.

\item All raw data obtained from the Twitter \ac{API} is stored in the Kafka pipeline for later processing. Each stored record consists of a key, the data and a timestamp. Kafka is configured to keep data for 3 hours, and after that time the system discards raw data automatically to save disk space. 

\item In another script called ``consumer.py'' we connect to a stream ``topic'' in Kafka to get tweets  by arrival order. This script is in turn  connected to 
the Apache Spark streaming system. The purpose of this script is to remove the data from the Kafka queue for processing. Thus, the producer and consumer
scripts implement an asynchronous	data pulling pipeline. 

\item The Spark streaming system collects the data from the consumer, and begins to clean data, filtering and keeping only real tweets and discarding re-tweets or replies. %Making use of spark tools we create ``data frame'' to process, manage and save data in a Hive database.

\item Data is saved in Hive tables at regular time intervals with specific information that include users, keywords, hashtags, geolocation and the original raw tweet. This data is used to future processes and searches.

\item Data stored in hive database is used to generate data visualizations and, in this project, to get data samples to train a  neural network models to measure the similarity between tweets.

\end{steps}

\begin{figure}[H]	
	\centering
	\includegraphics[width=150mm, scale = 1]{images/6_ths_system.png}	
	\caption{\ac{THS} Processing Architecture}	
	\label{figure:ths_system}
\end{figure}

\section{Strategy to Obtain the Model for Similarity \label{strategy}}
In order to obtain a model $M$ that can compute the similarity rank between two tweets, we follow the strategy in \cite{Wang2014}, and first train a bigger model $M'$
that is capable of classifying relative similarity in triplets of tweets. This is a common strategy in \ac{DL}: {\em train a bigger model that does a task in which the model you really want is  
a sub-task (sub-model)}. This is done because the bigger model might be easier to train on its intended task, or because there is data already available to train the bigger model.  Once the bigger model is trained, the sub-model is taken out and used in standalone fashion. In our specific case, the task that performs model $M$, computing the similarity between two tweets, becomes a sub-task in a bigger
model $M'$. In this case, $M'$ receives a triplet $(T_p, T_{h_1}, T_{h_2})$ and emits aclassification value as follows: 

\begin{itemize}
	\item 0 - tweet $T_{h_1}$ is more similar to $T_p$ than is tweet $T_{h_2}$.
	\item 1 - tweet $T_{h_2}$ is more similar to $T_p$ than is tweet $T_{h_1}$.
\end{itemize}
In order to train $M'$, {\em we need to first compute the similarity between pairs of tweets}, and this exactly what model $M$ will do. But it is much easier to ask humans to establish a rank between three tweets, as opposed as ranking many more tweets and trying to compare just two of them. The authors in \cite{Wang2014} used this same idea but for 
the case of computing image similarity. Our approach is to make make $M$ part of $M'$, 
train $M'$, and then extract the sub-components in the $M'$ that belong to $M$. 

\section{Labeling}
In order to work with the strategy presented in section \ref{strategy}, we created a data set for labeling. 
The dataset consisted of  4,225 triplets $(T_p, T_{h_1}, T_{h_2})$. All the tweets in these triplets were known to 
be related with a medical conditions, and had one of the five target keywords: Ebola, Zika, Flu, Measles, or Diarrhea.
There were 54 labelers, which were students from UPRM
that were enrolled in the course ``Introduction to Database Systems". Labelers were instructed to:
\begin{itemize}[nolistsep]
	\item compare $T_p$ with   $T_{h_1}$ and  $T_{h_2}$
	\item rank the similarity between $T_p$  and   $T_{h_1}$ on a scale in the range [0,4].
	\item rank the similarity between $T_p$  and   $T_{h_2}$ on a scale in the range [0,4].
\end{itemize}

\section{Building the Dataset for Training \label{buildtrain}}
In all of our data sets, the first tweet in the triplet , $T_p$, is always related with a disease. 
We took the data set from the previous section, and augmented it by adding two additional group of triplets.  In the first group, one of the 
hypothesis tweet, $T_{h_i}$ has the same disease keyword as $T_p$ but is not related with a medical condition (class 0). In this case, the 
ranking of this tweet with respect to $T_p$ is set to 0. In the second group, one of the 
hypothesis tweet, $T_{h_i}$ has a different disease keyword as that $T_p$.  In this case, the 
ranking of this tweet with respect to $T_p$ is also set to 0. In total, we grew the training data set to 11,425 tweets. 

The next step was to prepare the data and align it according to the model that we wanted to train. We took every triplet $(T_p, T_{h_1}, T_{h_2})$
and converted into corresponding tuples of the following shape:

\[
(T_p, D_{T_p}, M_{T_p}, T_{h_1}, D_{T_{h_1}}, M_{T_{h_1}},T_{h_2}, D_{T_{h_2}}, M_{T_{h_2}}, rank(T_p, T_{h_1}), rank(T_p, T_{h_2}), L)
\]
The structure of these tuples is as follows:
\begin{itemize}
	\item $T_p$ - premise tweet
	\item $D_{T_p}$ - index for the medical condition in $T_p$. Indices used: 0 - Flu , 1 - Ebola , 2 - Measles  , 3 - Diarrhea, 4 - Zika. 
	\item  $M_{T_p}$ - classification label of the tweet $T_p$: 0 - not related with a medical condition, 1 - related with a medical condition, 2 - ambiguous. 
	\item $T_{h_1}$ - first hypothesis tweet
	\item $D_{T_{h_1}}$ - index for the medical condition in $T_{h_1}$. Indices used: 0 - Flu , 1 - Ebola , 2 - Measles  , 3 - Diarrhea, 4 - Zika. 
	\item  $M_{T_{h_1}}$ - classification label of the tweet  $T_{h_1}$: 0 - not related with a medical condition, 1 - related with a medical condition, 2 - ambiguous. 
	\item $T_{h_2}$ - second hypothesis tweet
	\item $D_{T_{h_2}}$ - index for the medical condition in $T_{h_2}$. Indices used: 0 - Flu , 1 - Ebola , 2 - Measles  , 3 - Diarrhea, 4 - Zika. 
	\item  $M_{T_{h_2}}$ - classification label of the tweet $T_{h_2}$: 0 - not related with a medical condition, 1 - related with a medical condition, 2 - ambiguous. 
	\item $rank(T_p, T_{h_1})$ - similarity ranking between $T_p$ and  $T_{h_1}$ as given by the labelers.
	\item $rank(T_p, T_{h_2})$ - similarity ranking between $T_p$ and  $T_{h_2}$ as given by the labelers.
	\item $L$ -  classification label for relative comparison between tweets: 1 - if $rank(T_p, T_{h_1}) >= rank(T_p, T_{h_2})$, 0 - if  $rank(T_p, T_{h_1}) < rank(T_p, T_{h_2})$

\end{itemize}
%\section{How to Define Similarity}\label{define_similarity}
%
%The measure of similarity is represented by one (1) or zero (0). Its value depends of the relevance measures of triplets (group of three tweets). The measure of similarity is defined in the following function.
%
%\begin{equation}
%s(t_0, t_1, t_2) = 
%	\begin{cases} 
%		1 		& \text{if } r(t_0, t_1) \geq r(t_0, t_2) \\	
%		0 		& \text{if } r(t_0, t_1) < r(t_0, t_2)
%	\end{cases}
%\end{equation}
%
%\noindent Let $s(t_0, t_1, t_2)$ represent the similarity between  tweets in a triplet, when  $r(t_0, t_1) \geq r(t_0,t_2)$ is true then the value is 1, and when $r(t_0, t_1) < r(t_0,t_2)$ the value is 0; this means 1 if  $t_0$ is more similar to $t_1$ than it to $t_2$ and  0  if $t_0$ is more similar to $t_2$ than it to $t_1$.
%
%\begin{equation}
%r(t_i, t_j) = \sum^n_{k=1} \frac{m_k(t_i, t_j)}{n}, \text{where } m_k(t_i,t_j) \in \mathbb{N}, 1 \leq m(t_i,t_2) \leq 4
%\end{equation}
%
%\noindent The function of relevance $r(t_i, t_j)$ is calculated by two ways. The first way is estimated automatically because tweets have a marked difference in meaning, considering the disease class and their label value. In other hand when the value labels are calculated manually, the relevance is the average of sum each relevance measure of the pair of tweets, where the measure of relevance can take a value in a range of 1 to 4, depending of level of relevance of similarity. To higher value means more similarity between them. The process to labeling the relevance level and similarity of triplets are described in section \ref{data_relevance_labeling}

\section{Architecture of the Learning Model}\label{models}
The tuples described in the previous section are used in our method, and we train a model $M'$ on the following {\em classification task}: 
{\bf determine which tweet is more similar to $T_p$} : 
$T_{h_1}$ or $T_{h_2}$. In doing so, we will be training a sub-model $M$ that does a {\em regression task}: compute the $rank(T_p, T_{h_i})$ between the 
premise and a hypothesis. $M$ {\bf will become the model that is actually used in production to rank tweets}. 
Our approach  is a novel adaptation of a triplet-based network architecture \cite {Wang2014}, modified to classify if tweets are similar or not. It is based on the level of similarity relevance between tweets and their relatedness with a disease.  
The inputs to train the algorithm are given in \ac{CSV} file that contains tweets and the label values (see section \ref{data_labeling}) in the tuple form presented in section \ref{buildtrain}. Input data passes through a process to finally get the measures of similarity between a pair of tweets. 

%The entire process is shown in Figure \ref{figure:all_model}.
 
 Figure \ref{figure:all_model} depicts the organization of the \ac{DL} model that we use.
The  model starts with an embedding layer. Each of the tweets in a tuple passes through an embedding process to convert sentences into a matrix of multidimensional vectors, each row representing words. This embedding layer is a modified siamese network since it the same layer and weights are 
use to compute the embedding of the premise tweet $T_p$ and hypothesis tweets $T_{h_1}$ and $T_{h_2}$.  Next, we have a combiner layer, where we use different approaches such \ac{RNN} and \ac{CNN} to extract 
features from the data.
We use  several combinations of hyperparameters until get better results.

In the relevance layer we add a second input, called auxiliary input, that contains the information about the medical keyword of the tweet and the label indicating if the tweet is actually related with the medical condition or not. Again, we use the siamese network concept to pass this information for each tweet in the input.
 Using the auxiliary input and the output of the combiner layers, we calculate the relevance between the premise tweet and hypothesis tweet. This value becomes the rank of the premise and hypothesis. 
 
 Up to this point, the neural network has trained $M$, the model that computes the similarity ranking and the one we
  actually want in production. But  the process
 is not finished.
The final network component is the classification layer that is composed of a sequence of dense layers that end with a sigmoid classifier to finally get an output with value of 1 or 0. A label of  1 means that the first hypothesis  $T_{h_1}$ is more similar to the premise $T_p$ , and 0 means that the second hypothesis
$T_{h_2}$ is more similar to the premise $T_p$. This neural network is model $M'$ that we have referred as the ``bigger" model to be trained. Notice how 
training $M'$ indirectly trains model $M$, since we need to have the similarity ranks in order to make the comparison between $T_p, T_{h_1}$ and $T_{h_2}$, and emit the final label of the classification task. 
\begin{figure}[htbp]	
	\centering
	\includegraphics[width=140mm, scale = 1]{images/11_all_model.png}	
	\caption{Model Architecture}	
	\label{figure:all_model}
\end{figure}


\subsection{Input Setup}
The data obtained after the labeling step is prepared to be compatible with neural network input. The inputs to the neural network are the tweet, the disease type and disease-related labeling, and these inputs are required for each member of triplet $(T_p, T_{h_1},T_{h_2})$. Additionally, we need the labels for the relevance between premise and the two hypotheses and the final classification of similarity to define with hypothesis tweet is more related to premise tweet. 
The relevance between tweets become the secondary output of the network, whereas the classification label for the comparison is the main output. The total of triplets used like main input was 11425 triplets

Both disease group label and the disease-related labels form the auxiliary input. They were binarized to change it to a categorical kind, our data have four types of diseases and the labels to represent its relatedness with a disease condition, are three. In the next tweet we illustrate how is created the representation of the auxiliary input.

\begin{definition}
``It is march with streets are fully covered in snow I am drinking hot milk with cocoa butter and recovering from a flu I just turned my Christmas tree lights on yes, it is still here''
\end{definition}

In the example above, the tweet mentions ``flu'' and its meaning is related to the flu condition. Categorical representation is shown in figure \ref{figure:categorical}

\begin{figure}[H]	
	\centering
	\includegraphics[width=130mm, scale = 1]{images/10_categorical.png}	
	\caption{Auxiliary Input Representation}	
	\label{figure:categorical}
\end{figure}

\noindent Where the array with four elements is the type of disease and the array with three elements is the disease-related label. The values are binaries, it means they just can be zero or one. These two representations are combined and form the auxiliary input.

The main input is composed by the tweet which pass through a process of embedding. Our embedding layer use GloVe embedding \cite{Pennington2014}, an unsupervised algorithm to get a vector representation for each word in tweets. We used a pretrained vector that was trained with 2 billion of tweets. It is formed by a vocabulary of 1.2 million of terms with vectors of 25, 50, 100 and 200 dimensions. In the model were used 50 and 200 dimensions.

The main input was given in two ways. The first data with all text after labeling step. Second group of data go through an additional preprocessing step to remove stop words and lemmatized each word to the root lemma.

\subsection{Combiner Layer}

In this layer we implement three different combination of algorithms that include RNN and CNN approaches.  Each method was implemented with variation in the hyperparameters depending of each algorithm.
Find the best parameters combinations is not a trivial task, in first place we based our experiments in result of related researchers and then we create our own combination that is showed in Table \ref{Hyperparameters}

The hyperparameters variation was in epochs, learning rate, batch size and optimizer in training step. The parameters mentioned above are not learned by the network, those ones have predefined values set before training process.

\begin{table}[htb]
	\centering
	\begin{tabular}{||M{4.5cm}|M{4cm}||}
		\hline
		\textbf{Hyperparameters} 	& \textbf{Values} 	\\ \hline
		Learning rate           	& 0.001, 0.001      \\ \hline
		Epochs         				& 10, 20            \\ \hline
		Batch size           		& 32, 128 			\\ \hline		Optimizer                 	& RMSprop, Adam     \\ \hline	
	\end{tabular}
	\caption{Hyperparameters}\label{Hyperparameters}
\end{table}

\subsubsection{\ac{RNN} Combiner}
Using recurrent networks, we have two different combinations. The first implementation was a combination of two LSTM networks, they are networks with loops in them, allowing information to persist. The first LSTM network is conforming by 128 units and return all sequences by each module. The next LSTM network joined also has 128 units but only return the final sequence (accumulated output). The structure of this network is showed in Figure \ref{figure:lstm_only}

\begin{figure}[H]	
	\centering
	\includegraphics[width=150mm, scale = 1]{images/12_lstm_only.png}	
	\caption{LSTM network only}	
	\label{figure:lstm_only}
\end{figure}

Figure \ref{figure:lstm_only} shows a double \ac{LSTM} network, where embedded tweets are the inputs and the output is a tensor with the accumulate sequence. This process is the same for each embedding vector.

In other hand, the second network implementation is a variant of \ac{LSTM} networks. In this implementation we use twice sequential bidirectional LSTM networks, that consist in put two independent \ac{LSTM} networks, one in a normal time order, and the second in reverse time order. They are used to make predictions with both past and future context. The inputs for this layer in both networks are the embedded vectors, there are 3 vectors, one by each tweet in the triplet. The output of both two architectures are three tensors (a tensor is a data structure used how a representation of multidimensional vectors \cite{Tensor2019}), one by each tweet.

In the bidirectional network, inputs are the same that first implementation of \ac{LSTM} network. The outputs are concatenated at each time step. concatenated output of first network is delivered to next bidirectional network. The final output only returns a tensor with the accumulated sequences. The process is repeated by each embedding. The architecture is showed in Figure \ref{figure:bilstm}


\begin{figure}[H]	
	\centering
	\includegraphics[width=150mm, scale = 1]{images/13_bilstm.png}	
	\caption{BiLSTM network}	
	\label{figure:bilstm}
\end{figure}

  
\subsubsection{\ac{CNN} Combiner}

In this method we use the concept of inception networks. The figure of our inception network is showed next.

Figure xx


\subsection{Relevance Layer}

The input for this layer is the three outputs of combiner layer. Here the network tries to learn the relevance between tweets, the representation of first tweet with the second and with third respectively. To extract relationship between tweets, matching methods are applied to these two combinations, the method is a concatenation of premise representation, the hypothesis representation, the element-wise product and absolute element-wise difference of premise and hypothesis \cite{Conneau2017}. The resulting vector is merged with the auxiliary inputs of premise and hypothesis. Finally the output vector after concatenation is joined to a dense layer of 128 units with {ReLU} activation and end into a dense layer of one unit where the result represent the relevance between both tweets premise and hypothesis. Full structure of relevance component is showed in Figure \ref{figure:relevance}

\begin{figure}[H]	
	\centering
	\includegraphics[width=150mm, scale = 1]{images/14_relevance.png}	
	\caption{Relevance layer}	
	\label{figure:relevance}
\end{figure}

\subsection{Classification Layer}

Classification layer take as input the relevance layer outputs and concatenate them. Next it is following by a sequence of three dense layers of 16, 8 and 1 units respectively. Dense layer with 16 and 8 units have a \ac{ReLU} activation to convert results into positive data. In final dense layer we use a sigmoid activation to obtain a binary output. The final output can take two values, 1 means the first hypothesis is more similar to the premise and 0 when second hypothesis is more similar to premise. Figure \ref{figure:classification} show the architecture of this component.

\begin{figure}[H]	
	\centering
	\includegraphics[width=80mm, scale = 1]{images/15_classification.png}	
	\caption{Relevance layer}	
	\label{figure:classification}
\end{figure}

\section{Metrics}
\subsection{Accuracy}
Here text
\subsection{Mean Square Error}
Here text
\subsection{Loss}
here Text

\chapter{Performance Evaluation} \label{chapter 5}
\section{Hardware} 

In this research we use two different environments to run our experiments. The cluster distribution presented in figure \ref{figure:ths_cluster} shows the components of the \ac{THS} project that was configured like we described it in section \ref{ths_system}. The master-node and edge-node were set with Ubuntu 16.04 LTS \ac{OS} and data-nodes with Ubuntu 14.04 LTS \ac{OS}, these nodes are installed on bare metal. All operations are executed in the edge-node that connect the users with Big Data environment.

\begin{figure}[H]	
	\centering
	\includegraphics[width=150mm, scale = 1]{images/7_ths_cluster.png}	
	\caption{\ac{THS} cluster}	
	\label{figure:ths_cluster}
\end{figure}

The hardware specification by each node is showed following in table \ref{node_cluster}. The total cluster storage is 2.9 TB and 80 GB of memory to processing data, it does not include master-node and edge-node.

\begin{table}[htb]
	\centering
	\begin{tabular}{||M{3.0cm}|M{4.5cm}||}
		\hline
		\textbf{Hardware} 	& \textbf{Description} \\ \hline
		Hard disk           & 297 GB            \\ \hline
		RAM          		& 8 GB              \\ \hline
		Processor           & Intel(R) Xeon(R) E3120 @ 3.16GHz  \\ \hline
		GPU                 & None              \\ \hline	
	\end{tabular}
	\caption{\ac{THS} cluster node}\label{node_cluster}
\end{table}

The other environment with important resources to run our experiments was Chameleon Cloud, an online large-scale platform to research, with a variety of resources in Hardware and Software. To this project we use nodes with \ac{GPU} resources, these node ran in Ubuntu 16.04 LTS on bare metal. The image used for the project is a custom image created by \ac{THS} project. The hardware specification is described next in table \ref{node_chameleon}.

\begin{table}[htb]
	\centering
	\begin{tabular}{||M{3.0cm}|M{4.5cm}||}
		\hline
		\textbf{Hardware} 	& \textbf{Description} \\ \hline
		Hard disk           & 207 GB            \\ \hline
		RAM         		& 128 GB              \\ \hline
		Processor           & Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30 GHz x 2 \\ \hline
		GPU                 & Tesla P100 - 32GB              \\ \hline	
	\end{tabular}
	\caption{Chameleon Cloud custom node}\label{node_chameleon}
\end{table}

\section{Software}
The resources used for this research are the following: 
\begin{itemize}[nolistsep]
	\item \textbf{Python:} A programming language very used in research fields, for its easily and powerful. It is compatible with tools used in this research like Tensor Flow and Keras. All code in this project is written in Python language.
	
	\item \textbf{Twitter \ac{API}:} A tool that allow us get Twitter streaming data in real time for our research. 
	
	\item \textbf{\ac{HDFS}:} A software that allows  distribute processing a  massive amount of data, running in commodity hardware \cite{Hadoop2019}.
	
	\item \textbf{HIVE:} A data warehouse software for large datasets in a distributed storage. For querying use \ac{SQL} syntax \cite{Hive2019}.
	
	\item \textbf{KAFKA:} It is a streaming platform to storage stream of records and allows to process it as they occur. It can be implemented in standalone mode or cluster mode. Each stream store is called ``topics'' \cite{Kafka2019}.
	
	\item \textbf{SPARK:} Apache Spark is a fast, a general purpose and a unified analytic engine for large-scale data processing. It is compatible with JAVA, Scala, Python and R. Spark support structured data processing, \ac{ML}, graph processing and stream processing of live data \cite{Spark2019}.
	
	\item \textbf{Tensorflow:} Open source \ac{ML} library for building and training complex neural network models \cite{Tensor2019}.
	
	\item \textbf{Keras:} An neural network \ac{API}, that allows easy and fast experimentation, support \ac{CNN} and \ac{RNN}. This tool work with python and can be used on \ac{CPU} and \ac{GPU} \cite{Keras2019}.
	
	\item \textbf{\ac{CUDA}:} A programming model developed by NVIDIA, it is a parallel computing programming on \ac{GPU}, the sequential computes on thousands of cores in parallel mode, optimizing the performance of tasks \cite{Cuda2019}.
	
	\item \textbf{Torch: } A framework to support \ac{ML} algorithms using \ac{GPU} in a efficient way. This tool seeks to facilitate and speed up the implementation of neural network libraries and optimization packages \cite{Torch2019}.\\
	
\end{itemize}


Tools used are Open Source. Hadoop, Yarn, Spark, Kafka for \ac{THS} cluster only and Keras, TensorFlow, Tensor Board, Scikit-learn, \ac{NLTK}, SciPy and torch for both {THS} edge-node and Chameleon Cloud custom node. Versions of Big Data tools implemented in \ac{THS} project are showed in table \ref{software_cluster}. 

\begin{table}[htb]
	\centering
	\begin{tabular}{||M{3.0cm}|M{4.0cm}||}
		\hline
		\textbf{Software} 	& \textbf{Version} \\ \hline
		Hadoop \& Yarn   	& 2.7              \\ \hline
		Hive            	& 2.2              \\ \hline
		Spark             	& 2.1              \\ \hline
		Kafka            	& 0.10.1           \\ \hline
	\end{tabular}
	\caption{Big Data tools in \ac{THS} system \label{software_cluster}} 
\end{table}

Versions of software packages installed in both environments are presented in table \ref{software_packages}.

\begin{table}[htb]
	\centering
	\begin{tabular}{||M{3.0cm}|M{4.0cm}||}
		\hline
		\textbf{Software} 	& \textbf{Version} 	\\ 	\hline
		TensorFlow       	& 1.12.0            \\ 	\hline
		TensorBoard     	& 1.12.2            \\ 	\hline
		Keras            	& 2.2.2            	\\ 	\hline
		Scikit-learn     	& 0.19.1           	\\ 	\hline
		\ac{NLTK}           & 3.4           	\\ 	\hline
		SciPy         		& 1.1.0           	\\ 	\hline
		Torch           	& 1.0.1          	\\ 	\hline
		\ac{CUDA}			& 9.0				\\	\hline
	\end{tabular}
	\caption{Version of software in nodes \label{software_packages}} 
\end{table}

\section{Data Collection}

The process to collect data and prepare it until it been transformed in the input of our model is shown in Figure \ref{figure:data_collection} and it is described following.

\begin{figure}[H]	
	\centering
	\includegraphics[width=150mm, scale = 1]{images/8_data_collection.png}	
	\caption{Data Collection Process}	
	\label{figure:data_collection}
\end{figure}

\begin{steps}
	
	\item To get data from twitter we use a Twitter Streaming \ac{API} and we collect data and it is process through \ac{THS} system where raw tweets are stored in a Hive Database.
	
	\item Running a python script we get data from ``raw$\_$tweet'' table of \ac{THS} system. Here we filter data by specific diseases and by date of stored using Spark tools to be following save it in a \ac{JSON} file because originally tweets have a \ac{JSON} format. It facilitates the next step of processing.
	
	\item Using the created \ac{JSON} file in previous step, we filter data and only take the tweet text (sentence itself) because our project only needs to work with the text written by the user. Also, we throw away repeated tweets. This filtered data is saved in a \ac{CSV} file to next processing step.
	
	\item In this step the data pass for the pre-processing step to clean and filter data before deliver data to labelling step. This process is described in section \ref{data_preprocessing} 
	
	\item Data collected is ready to be used like input of our model.
	
\end{steps}

\section{Data Pre-processing} \label{data_preprocessing}

Data collected from social networks contains misspelling, slang or no textual information, that makes it less structured and informal. This is a reason to clean data like a previous step to build our model. This stage is composed by three demarcated steps: Pre-processing before labeling, labeling task and final pre-processing. These steps are showing in Figure \ref{figure:data_preprocessing}. 

\begin{figure}[H]	
	\centering
	\includegraphics[width=150mm, scale = 0.8]{images/9_data_preprocessing.png}	
	\caption{Data Pre-processing}	
	\label{figure:data_preprocessing}
\end{figure}

In this section we will describe the first and last step of processing. The labeling step will be described in section \ref{data_labeling}. Before use tweets directly to labeling, it is necessary to filter and clean data \cite{Dai2017} \cite{Ahuja2017} \cite{Halibas2018}. The operations to prepare data is described following.

\begin{itemize}[nolistsep]
	\item \textbf{Remove line breaks:} It is necessary clean it because it can be generating more than one row from same tweet and we can lose context and meaning.
	
	\item \textbf{Remove links, hashtags and mentions:} Social media are informal and contains no textual information that is why we clean no-relevant data like links, hashtags and mentions \cite{Dai2017}.
	
	\item \textbf{Character unespace:} We replace source code based on hexadecimal code with the character that it represents to be understandable and compatible with human language.
	
	\item \textbf{Reduce lengthening:} Many typo errors are presented in text like repetition of an char many times to express emphasis or simply because exist a typographic mistake. For example, the word ``fluuu'' represent the next word ``flu''. In cases like this one is needed fix it and replace it to the right word.
	
	\item \textbf{Expand contractions:} To ensure a correct representation of each word we expanded all contraction to their original form.
	
	\item \textbf{Keep text and Number:} After of previous operation we remove all characters which are noisy to \ac{ML} algorithm. We just conserve text and numbers.
	
	\item \textbf{Spelling corrections:} It is necessary a spelling corrector to fix some unknown words and help to avoid misunderstanding of meaning in labeling step or in the process of vectorization avoid being processed as an unknown word.\\
	
\end{itemize}

Cleaned data got from previous process is ready to deliver to labeling users, where main and auxiliary label inputs are built. This step is described in section \ref{data_labeling}.

The final \ac{NLP} step is showed following.

\begin{itemize}[nolistsep]
	\item \textbf{Lemmatization:} It is a process to find a lemma for each word in a tweet. That refers to convert all inflected forms in its base form, for example the words: “taking”, “took” refers to base form “to take”. Lemmatization considers the context and meaning to convert a word in its base form, for example in the next word “caring” a simple stemming process will cutoff termination “ing” and return “car”. In other hand with lemmatization it returns “care”. The more known tools are \ac{NLTK}, textBlob, spaCy, pattern, Stanford CoreNLP and others. 

	\item \textbf{Remove stop-words:} The final operation of processing step is removing the stop-words and just keep words with high meaning.
\end{itemize}

\section{Data Labeling} \label{data_labeling}

Due our model follows a supervised approach; it is necessary use labeled data. In this project we have two different kinds of labeling. First one is focused in classify tweets if they are related or no to a disease. The other type refers to set a measure of similarity between tweets. How they were built are described following.

\subsection{Disease-related labeling}

This training set is composed of 11,937 tweets and it was labeled by five members of \ac{THS} team. Labels depends of the classification in term of similar meaning and their relatedness with specific topics about health diseases. There are three possible label values by each tweet. 

\begin{itemize} [nolistsep]
	\item \textbf{0:} This label represents a tweet that is not related with a disease. 
	\item \textbf{1:} It is a tweet which the meaning has related with a disease.
	\item \textbf{2:} Tweets have an ambiguous meaning, difficult to classify.
\end{itemize}

\subsection{Labeling for level of relevance} \label{data_relevance_labeling}

To label relevance of similarity on tweets we built triplets that are a collection of three sets. The set that was manually labeled is composes by three tweets with a common meaning, it belongs to the same disease class and their label value is 1 (``the tweet meaning is related to a disease''). The second group is form by two similar tweets and one tweet out of class, that means that last tweet is in a different class that others two; consequently, the first and second are similar, in other hand the first and the third are not similar. Finally, the last group of triplets is composed by tweets with the same class of disease, but the third tweet has a label value of 0 or 2, indicating that the tweet does not has a semantic or context relationship with the others tweets in the triplet.

All triplets were built using the training set of 11,937 samples after labeling if it is related with a disease or no. The triplets that were labeled manually are which belongs to the first group, only tweets in the same class with label with value of 1, to know the level of relevance between similar tweets. The total of triplets labeled were 4,225; it was 1,075 triplets by each disease mentioned next ``Flu'', ``Measles'', ``Ebola'' and ``Diarrhea''. ``Zika'' was not taken in because, we do not have enough samples to build triplets. Each tweet was labeled by 3 different subjects doing a total of 12,675 samples. Triplets were equality divided between 54 collaborators.  The measure of relevance is in a range of 1 to 4, where the level of final relevance is given by the average of those three measures. While the measure is higher, it represents more similarity between the pair of tweets analyzed. 

A metric of similarity is obtained from relevance measures. Given three tweets $t_0$, $t_1$ and $t_2$, the measure of similarity takes a value of 1 or 0; where this is 1 when $t_0$ is more similar to $t_1$ than $t_2$, and it is 0 when $t_0$ is more similar to $t_2$ than $t_1$.

The rules that was considered to manual labeling of relevance between tweets are the following:



\begin{enumerate}
	\item \text{When tweets talk about the same disease:}
	\begin{itemize}
		\item \textbf{Tweet 1:} \textit{There is a measles outbreak in japan.}
		\item \textbf{Tweet 2:} \textit{Japanese fear measles outbreak in progress.}
	\end{itemize}

	\item \text{Tweets talk about the same disease, but the events are not related:}
	\begin{itemize}
		\item \textbf{Tweet 1:} \textit{Government intensifies educational campaign against Ebola.}
		\item \textbf{Tweet 2:} \textit{Many countries launch initiatives to raise awareness about Ebola.}
	\end{itemize}

	\item \text{Tweets talk about of the same disease at the same location:}
	\begin{itemize}
		\item \textbf{Tweet 1:} \textit{Outbreak report of Zika at Ponce.}
		\item \textbf{Tweet 2:} \textit{Government of Ponce warns about outbreak of Zika.}
	\end{itemize}


	\item \text{Describe different symptoms of a same disease:}
	\begin{itemize}
		\item \textbf{Tweet 1:} \textit{Big headache and fever due do you the flu.}
		\item \textbf{Tweet 2:} \textit{I have been two days in bed feeling ill, due the flu.}
	\end{itemize}


	\item \text{Do a similar action in topics related to diseases:}
	\begin{itemize}
		\item \textbf{Tweet 1:} \textit{Government recommends getting the flu vaccine.}
		\item \textbf{Tweet 2:} \textit{Today I went to the pharmacy to get the flu vaccine.}
	\end{itemize}
\end{enumerate}

\section{Experiment Setup}
Experiments was set in two environments with different resources. Chameleon Cloud and \ac{THS} cluster. All code was developed in Python, the script permits us to collect the data from twitter using \ac{THS} system and it was pre-processing using \ac{NLP} techniques before and after labeling of data. The process of labeling has two steps, the first step is grouping the tweets by disease and label it by their semantic relation with a disease or no. The second type of labeling is related with the compute of a measure of similarity, using triplets to compare de similarity between them and get a metric of relevance that tell us what of two tweets is more like the first one.

These two-input feed our created model. The pair of tweets referred like premise and hypothesis pass by a series of transformation to extract relations between them and finally get a measure of relevance. Along the process we use \ac{DL} approaches using \ac{LSTM} and \ac{CNN} with different variations explained in section \ref{models}, to finally build a model able to determine given two tweets how similar they are between themselves.

A perspective overview of the setup structure of whole project is showed in Figure \ref{figure:model_setup}

\begin{figure}[H]	
	\centering
	\includegraphics[height=190mm,  scale = 0.8]{images/10_architecture_setup.png}	
	\caption{Model Setup Overview}	
	\label{figure:model_setup}
\end{figure}

\section{Experimental Results and Discussion}


\chapter{Conclusion and Future Work} \label{chapter 6}
Summary, text here

%\chapter*{References} 
%\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{unsrt}
\bibliography{references} 
%\printbibliography[heading=none, ]

\begin{appendices}
\chapter{GitHub Repositories}
The GitHub repositories of the big data and machine learning deamon are avaliable upon request at danny.villanueva1@upr.edu. The following sections contain the links.

\section{Big Data Platform}
https://github.com/THSUPRM/bigdata/tree/master/python

\subsection{Machine Learning Platform}
https://github.com/THSUPRM/bigdata/tree/master/DetectDiseaseTHS/ths
\end{appendices}

\end{document}